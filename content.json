[{"title":"Pointnet++ 网络个人笔记","date":"2020-03-15T21:00:00.000Z","path":"2020/03/16/pointne2学习笔记/","text":"Pointnet++个人笔记前言Pointnet提取的全局特征能够很好地完成分类任务，由于网络将所有的点最大池化为了一个全局特征，因此局部点与点之间的联系并没有被网络学习到，导致网络的输出缺乏点云的局部结构特征，因此PointNet对于场景的分割效果十分一般。在点云分类和物体的Part Segmentation中，这样的问题可以通过中心化物体的坐标轴部分地解决，但在场景分割中很难去解决。 因此作者在此基础上又提出了能够实现点云作多层特征提取的Pointnet++网络，网络结构如下： ​ Qi et al. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. NIPS 2017. 网络的基本组成下面介绍上图中的网络设计，传统的CNN在进行特征学习时，使用卷积核作为局部感受野，每层的卷积核共享权值，进过多层的特征学习，最后的输出会包含图像的局部特征信息。通过改变中借鉴CNN的采样思路，采取分层特征学习，即在小区域中使用点采样+成组+提取局部特征（S+G+P）的方式，包含这三部分的机构称为Set Abstraction。 Sampling：利用FPS（最远点采样）随机采样点； Grouping：利用Ball Query划一个R为半径的圈，将每个圈里面的点云作为一簇； Pointnet: 对Sampling+Grouping以后的点云进行局部的全局特征提取。 以2D点图为例，整个SA(Set Abstraction)三步的实现过程表示如下： 每层新的中心点都是从上一层抽取的特征子集，中心点的个数就是成组的点集数，随着层数增加，中心点的个数也会逐渐降低，抽取到点云的局部结构特征。 ## 针对非均匀点云情况当点云不均匀时，每个子区域中如果在分区的时候使用相同的球半径，会导致部分稀疏区域采样点过小。 文中提出多尺度成组 (MSG)和多分辨率成组 (MRG)两种解决办法。 简单概括这两种采样方法： 多尺度成组（MSG）：对于选取的一个中心点设置多个半径进行成组，并将经过PointNet对每个区域抽取后的特征进行拼接（concat）来当做该中心点的特征，个人认为这种做法会产生很多特征重叠，结果会可以保留和突出（边际叠加）更多局部关键的特征，但是这种方式不同范围内计算的权值却很难共享，计算量会变大很多。 多分辨率成组（MRG）：对不同特征层上（分辨率）提取的特征再进行concat，以上图右图为例，最后的concat包含左右两个部分特征，分别来自底层和高层的特征抽取，对于low level点云成组后经过一个pointnet和high level的进行concat，思想是特征的抽取中的跳层连接。当局部点云区域较稀疏时，上层提取到的特征可靠性可能比底层更差，因此考虑对底层特征提升权重。当然，点云密度较高时能够提取到的特征也会更多。这种方法优化了直接在稀疏点云上进行特征抽取产生的问题，且相对于MSG的效率也较高。 在该网络中作者使用了对输入点云进行随机采样(丢弃)random input dropout(DP)方法。Dropout的设计本身是为了降低过拟合，增强模型的鲁棒性，结果显示对于分类任务的效果也有不错的提升，作者给了一个对比图： 本文中使用的缩写说明： SA：set abstraction 点集抽取模块 FC：fully connected layers 全连接层 FP：featurepropagation 特征传播模块（跨层连接，多个全连接） SA模块的代码实现 utils/pointnet_util.py/ 中采样成组的代码具体实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190def sample_and_group(npoint, radius, nsample, xyz, points, knn=False, use_xyz=True): ''' 输入参数说明： Input: npoint: int32，中心点的数量（分组数） radius: float32，ball quary的球半径大小 nsample: int32，区域内采样到的点数 xyz: (batch_size, ndataset, 3) TF tensor，例如：分类任务起始值（32，1024，3） points: (batch_size, ndataset, channel) TF tensor, 如果为None则等于xyz knn: bool, True表示使用KNN方法采样，否则使用球半径搜索 use_xyz: bool, True 表示抽取的局部点的特征与xyz进行concat, 否则不进行，默认为True 输出参数说明： Output: new_xyz: (batch_size, npoint, 3) TF tensor new_points: (batch_size, npoint, nsample, 3+channel) TF tensor,点的特征进行了concat idx: (batch_size, npoint, nsample) TF tensor, 采样的局部区域内点的索引值 grouped_xyz: (batch_size, npoint, nsample, 3) TF tensor, 通过减去xyz对点进行区域归一化 注：源码中没有tf_ops/grouping和sampling/下没有放编译生成对应的链接库.so文件，可能要重新编译才能执行相应的py脚本 ''' #1.对原始点云输入进行采样和分组 new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz)) # (batch_size, npoint, 3) if knn: _,idx = knn_point(nsample, xyz, new_xyz) else: idx, pts_cnt = query_ball_point(radius, nsample, xyz, new_xyz) grouped_xyz = group_point(xyz, idx) # (batch_size, npoint, nsample, 3) grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1]) # translation normalization，减去中心点坐标进行区域坐标归一化 #2.对高层次特征进行分组 if points is not None: grouped_points = group_point(points, idx) # (batch_size, npoint, nsample, channel) if use_xyz: new_points = tf.concat([grouped_xyz, grouped_points], axis=-1) # (batch_size, npoint, nample, 3+channel) else: new_points = grouped_points else: new_points = grouped_xyz return new_xyz, new_points, idx, grouped_xyz#在最后一次SA操作中，需要对全部特征进行采样分组def sample_and_group_all(xyz, points, use_xyz=True): ''' #输出变为三个参数，功能同上 Inputs: xyz: (batch_size, ndataset, 3) TF tensor points: (batch_size, ndataset, channel) TF tensor use_xyz: bool 输出： Outputs: new_xyz: (batch_size, 1, 3) as (0,0,0) new_points: (batch_size, 1, ndataset, 3+channel) TF tensor Note: 等价于sample_and_group（npoint=1, radius=inf）以(0,0,0)为重心 ''' batch_size = xyz.get_shape()[0].value nsample = xyz.get_shape()[1].value new_xyz = tf.constant(np.tile(np.array([0,0,0]).reshape((1,1,3)), (batch_size,1,1)),dtype=tf.float32) # (batch_size, 1, 3) idx = tf.constant(np.tile(np.array(range(nsample)).reshape((1,1,nsample)), (batch_size,1,1))) grouped_xyz = tf.reshape(xyz, (batch_size, 1, nsample, 3)) # (batch_size, npoint=1, nsample, 3) if points is not None: if use_xyz: new_points = tf.concat([xyz, points], axis=2) # (batch_size, 16, 259) else: new_points = points new_points = tf.expand_dims(new_points, 1) # (batch_size, 1, 16, 259) else: new_points = grouped_xyz return new_xyz, new_points, idx, grouped_xyzdef pointnet_sa_module(xyz, points, npoint, radius, nsample, mlp, mlp2, group_all, is_training, bn_decay, scope, bn=True, pooling='max', knn=False, use_xyz=True, use_nchw=False): ''' PointNet Set Abstraction (SA) Module Input: xyz: (batch_size, ndataset, 3) TF tensor points: (batch_size, ndataset, channel) TF tensor npoint: int32 -- 最远点采样点数（中心点数/成组数） radius: float32 -- 局部区域的搜索半径 nsample: int32 -- 每个区域内的采样点数 mlp: list of int32 -- 对每个点进行MLP的网络（输出）大小 mlp2: list of int32 -- 对每个区域进行MLP的网络（输出）大小 group_all: bool -- 如果为True,则重写npoint, radius and nsample为None use_xyz: bool, True 表示抽取的局部点的特征与xyz进行concat, 否则不进行 use_nchw: bool, True, 使用NCHW点云数据格式进行卷积, 作者指出这样比NHWC格式的计算更快 Return: new_xyz: (batch_size, npoint, 3) TF tensor new_points: (batch_size, npoint, mlp[-1] or mlp2[-1]) TF tensor idx: (batch_size, npoint, nsample) int32 -- 区域索引 ''' data_format = 'NCHW' if use_nchw else 'NHWC' with tf.variable_scope(scope) as sc: # Sample and Grouping if group_all: nsample = xyz.get_shape()[1].value new_xyz, new_points, idx, grouped_xyz = sample_and_group_all(xyz, points, use_xyz) else: new_xyz, new_points, idx, grouped_xyz = sample_and_group(npoint, radius, nsample, xyz, points, knn, use_xyz) # Point Feature Embedding if use_nchw: new_points = tf.transpose(new_points, [0,3,1,2])#nchw-&gt;nwch for i, num_out_channel in enumerate(mlp): new_points = tf_util.conv2d(new_points, num_out_channel, [1,1], padding='VALID', stride=[1,1], bn=bn, is_training=is_training, scope='conv%d'%(i), bn_decay=bn_decay, data_format=data_format) if use_nchw: new_points = tf.transpose(new_points, [0,2,3,1])#nchw-&gt;nhwc \"\"\" 省略 some code(区域max pooling) \"\"\" #针对稀疏点云加入多尺度采样（msg）def pointnet_sa_module_msg(xyz, points, npoint, radius_list, nsample_list, mlp_list, is_training, bn_decay, scope, bn=True, use_xyz=True, use_nchw=False): ''' PointNet Set Abstraction (SA) module with Multi-Scale Grouping (MSG) Input: xyz: (batch_size, ndataset, 3) TF tensor points: (batch_size, ndataset, channel) TF tensor npoint: int32 -- #points sampled in farthest point sampling radius: list of float32 -- search radius in local region nsample: list of int32 -- how many points in each local region mlp: list of list of int32 -- output size for MLP on each point use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format Return: new_xyz: (batch_size, npoint, 3) TF tensor new_points: (batch_size, npoint, \\sum_k&#123;mlp[k][-1]&#125;) TF tensor ''' data_format = 'NCHW' if use_nchw else 'NHWC' with tf.variable_scope(scope) as sc: new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz)) new_points_list = [] for i in range(len(radius_list)): radius = radius_list[i] nsample = nsample_list[i] idx, pts_cnt = query_ball_point(radius, nsample, xyz, new_xyz) grouped_xyz = group_point(xyz, idx) grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1]) if points is not None: grouped_points = group_point(points, idx) if use_xyz: grouped_points = tf.concat([grouped_points, grouped_xyz], axis=-1) else: grouped_points = grouped_xyz if use_nchw: grouped_points = tf.transpose(grouped_points, [0,3,1,2]) for j,num_out_channel in enumerate(mlp_list[i]): grouped_points = tf_util.conv2d(grouped_points, num_out_channel, [1,1], padding='VALID', stride=[1,1], bn=bn, is_training=is_training, scope='conv%d_%d'%(i,j), bn_decay=bn_decay) if use_nchw: grouped_points = tf.transpose(grouped_points, [0,2,3,1]) new_points = tf.reduce_max(grouped_points, axis=[2]) new_points_list.append(new_points) new_points_concat = tf.concat(new_points_list, axis=-1) return new_xyz, new_points_concatdef pointnet_fp_module(xyz1, xyz2, points1, points2, mlp, is_training, bn_decay, scope, bn=True): ''' PointNet Feature Propogation (FP) Module FP层，作用是更新从插值操作和跳层连接合并来的特征 Input: xyz1: (batch_size, ndataset1, 3) TF tensor xyz2: (batch_size, ndataset2, 3) TF tensor, sparser than xyz1 points1: (batch_size, ndataset1, nchannel1) TF tensor points2: (batch_size, ndataset2, nchannel2) TF tensor mlp: list of int32 --对给个点进行mlp后的输出特征维度大小 Return: new_points: (batch_size, ndataset1, mlp[-1]) TF tensor 注：这一部分会用到插值模块，源码中带有tf_ops/3d_interpolation/tf_interpolate_so.so文件可以使用，不用重新编译。不同于需要进行编译的grouping和sampling操作。 ''' with tf.variable_scope(scope) as sc: dist, idx = three_nn(xyz1, xyz2) dist = tf.maximum(dist, 1e-10) norm = tf.reduce_sum((1.0/dist),axis=2,keep_dims=True) norm = tf.tile(norm,[1,1,3]) weight = (1.0/dist) / norm interpolated_points = three_interpolate(points2, idx, weight) if points1 is not None: new_points1 = tf.concat(axis=2, values=[interpolated_points, points1]) # B,ndataset1,nchannel1+nchannel2 else: new_points1 = interpolated_points new_points1 = tf.expand_dims(new_points1, 2) for i, num_out_channel in enumerate(mlp): new_points1 = tf_util.conv2d(new_points1, num_out_channel, [1,1], padding='VALID', stride=[1,1], bn=bn, is_training=is_training, scope='conv_%d'%(i), bn_decay=bn_decay) new_points1 = tf.squeeze(new_points1, [2]) # B,ndataset1,mlp[-1] return new_points1 以上是SA和FP部分的代码实现，接下来对分类任务的代码进行解读。 单尺度成组(SSG)分类网络的实现以最基础的单尺度采样分组设计为例，结合代码了解模型的搭建过程。 models/pointnet2_cls_ssg.py / 1234567891011121314151617181920212223242526272829303132333435def get_model(point_cloud, is_training, bn_decay=None): \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\" batch_size = point_cloud.get_shape()[0].value num_point = point_cloud.get_shape()[1].value end_points = &#123;&#125; l0_xyz = point_cloud l0_points = None end_points['l0_xyz'] = l0_xyz # Set abstraction layers # Note: When using NCHW for layer 2, we see increased GPU memory usage (in TF1.4). # So we only use NCHW for layer 1 until this issue can be resolved. \"\"\" 调用三次SA模块+三次全连接层+两次dropout=0.5，和PointNet一样，除最后一层外，在所有的全连接层后都会进行批量归一化操作+ReLU操作： SA(512, 0.2, [64, 64, 128]) → SA(128, 0.4, [128, 128, 256]) → SA([256, 512, 1024]) →FC(512, 0.5) → FC(256, 0.5) → FC(K) \"\"\" l1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points, npoint=512, radius=0.2, nsample=32, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1', use_nchw=True) l2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points, npoint=128, radius=0.4, nsample=64, mlp=[128,128,256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2') l3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points, npoint=None, radius=None, nsample=None, mlp=[256,512,1024], mlp2=None, group_all=True, is_training=is_training, bn_decay=bn_decay, scope='layer3') # Fully connected layers net = tf.reshape(l3_points, [batch_size, -1]) net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay) net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1') net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training, scope='fc2', bn_decay=bn_decay) net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp2') net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3') return net, end_points \"\"\" 对于多尺度的分类网络模型（MSG）对应于pointnet2_cls_msg.py，这里的半径和mlp维度都分别转变为向量和数组表示形式，整体的计算过程如下： SA(512, [0.1, 0.2, 0.4], [[32, 32, 64], [64, 64, 128], [64, 96, 128]]) →SA(128, [0.2, 0.4, 0.8], [[64, 64, 128], [128, 128, 256], [128, 128, 256]]) →SA([256, 512, 1024]) → F C(512, 0.5) → F C(256, 0.5) → F C(K) 对于多分辨率分类模型（MRG），作者在附录中只是给出了设计的步骤，实现源码没有给出 \"\"\" 文章给出了针对ModelNet40S数据集上的分割模型的效果比较： 相比于Pointnet的结果，Pointnet++在此有小幅度的提升。 对于分割部分，会单独进行一次总结，文中给出的分割效果对比图： 结果显示在场景分割网络中，准确度关系为：MSG+DP &gt; MRG+DP &gt; SSG&gt; PointNet 源码其余部分的介绍不详细展开，根据个人理解将源码的结构与功能设计展示如下： 结语本文主要结合代码层面总结了pointnet++网络设计以及分类任务的实现。重点理解pointnet++是如何利用set abstraction(SA)这种结构学习到局部结构上的特征，并通过跳步连接和多尺度采样（MSG+DP）来提高模型对点云的分割准确性。可以注意到pointnet++中在特征提取时使用pointnet网络，但是最后的结果的鲁棒性在不添加其他设计的情况下没有原网络好，并且作者没有继续使用T-net进行点云对齐的方法。接下来将在此基础上继续进行更多相关论文的学习。 源码地址：1.原论文实现代码https://github.com/charlesq34/pointnet22.基于pytorch实现：https://github.com/erikwijmans/Pointnet2_PyTorchhttps://github.com/yanx27/Pointnet_Pointnet2_pytorch","link":"","categories":[{"name":"Pointcloud,DL","slug":"Pointcloud-DL","permalink":"https://Yansz.github.io/categories/Pointcloud-DL/"}],"tags":[]},{"title":"Pointnet网络个人笔记","date":"2020-03-09T21:00:00.000Z","path":"2020/03/10/pointnetcode/","text":"前言Pointnet开创性地将深度学习直接用于三维点云任务。由于点云数据的无序性，无法直接对原始点云使用卷积等操作。Pointnet提出对称函数来解决点的无序性问题，设计了能够进行分类和分割任务的网络结构，本文结合源码与个人的理解对于T-net网络和对称函数进行分析。 点的无序性针对点的无序性问题实际上是文章提出了三个方案： 对于无序点集进行排序（Pointcnn）。 把点集当做序列进行处理，但是这种方法需要对输入点集做所有的排列变换进行数据增强。 使用对称函数，Pointnet使用的就是这种方法。 Pointnet网络主要使用对称函数解决点的无序性问题，对称函数就是指对输入顺序不敏感的函数。如加法、点乘、max pooling等操作。假设输入特征为NxD，N表示点数，D表示维度数，在max pooling作用下，取出每个维度上最大值的1xD的向量，每一维特征都与其顺序无关，这样便保证了对于点云输入顺序的鲁棒性。 点云的旋转不变性Pointnet的解决方法是学习一个变换矩阵T，即T-Net结构。由于loss的约束，使得T矩阵训练会学习到最有利于最终分类的变换，如把点云旋转到正面。论文的架构中，分别在输入数据后和第一层特征中使用了T矩阵，大小为3x3和64x64。其中第二个T矩阵由于参数过多，考虑添加正则项，使其接近于正交矩阵，减少点云的信息丢失。 1. T-Net网络结构将输入的点云数据作为nx3x1单通道图像，接三次卷积和一次池化后，再reshape为1024个节点，然后接两层全连接，网络除最后一层外都使用了ReLU激活函数和批标准化（batch normalization）。 论文中的T-net网络的实际结构并不复杂，我根据个人理解画出T-net的结构。 实际训练过程中，T矩阵的参数初始化使用单位矩阵（np.eye(K)）， 参数会随着整个网络的训练进行更新，并不是提前单独训练的。很多文章提到T-Net对特征进行对齐，保证了模型的对特定空间转换的不变性，我其实不太理解这种说法。 实际上通过网络结构看出T-net结构是一个mini的Pointnet做特征提取，是个弱监督学习设计，我理解为需要训练一个矩阵对输入点（或者深层特征）进行坐标吧变换，个人认为这样的设计实际上是可以保留原始点云的部分特征，为后面的concat操作提供更多特征。源码中在点云分类部分使用到了T-net，点云分割部分可以不用，对结果并没有太大的提升，原因在于pointnet结构自身不能学到点云点的局部联系，因此即使加入类似结构的T-net也是一样。 models/transform_nets.py中的网络实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def input_transform_net(point_cloud, is_training, bn_decay=None, K=3): \"\"\" Input： BxNx3 B=batch size;N=number of pointcloud Output: 3x3 matrix \"\"\" batch_size = point_cloud.get_shape()[0].value num_point = point_cloud.get_shape()[1].value input_image = tf.expand_dims(point_cloud, -1) # 扩展一维表示通道C，BxNx3x1 # 输入BxNx3x1 # 64个1x3卷积核 （参数[1,3]定义） # 移动步长 1x1 （stride=[1,1]定义） # 输出 BxNx1x64 net = tf_util.conv2d(input_image, 64, [1,3], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='tconv1', bn_decay=bn_decay) # 输入 BxNx1x64 # 128个 1x1 卷积核 # 步长 1x1 # 输出 BxNx1x128 net = tf_util.conv2d(net, 128, [1,1], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='tconv2', bn_decay=bn_decay) # 输入 BxNx1x128 # 1024个 1x1卷积核 # 输出 BxNx1x1024 net = tf_util.conv2d(net, 1024, [1,1], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='tconv3', bn_decay=bn_decay) # 池化操作 # 输入 BxNx1x1024 # 输出 Bx1x1x1024 net = tf_util.max_pool2d(net, [num_point,1], padding='VALID', scope='tmaxpool') # 输出 Bx1024 net = tf.reshape(net, [batch_size, -1]) # 全连接层 # 输入 Bx1024 # 权重矩阵 1024x512 # 偏置 512x1 # 输出 Bx512 net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training, scope='tfc1', bn_decay=bn_decay) # 全连接 # 输出 Bx256 net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training, scope='tfc2', bn_decay=bn_decay) # 再次使用全连接,不加ReLU和BN # 输出 Bx9 with tf.variable_scope('transform_XYZ') as sc: assert(K==3) weights = tf.get_variable('weights', [256, 3*K], initializer=tf.constant_initializer(0.0), dtype=tf.float32) biases = tf.get_variable('biases', [3*K], initializer=tf.constant_initializer(0.0), dtype=tf.float32) #初始化为3x3单位矩阵 biases += tf.constant([1,0,0,0,1,0,0,0,1], dtype=tf.float32) transform = tf.matmul(net, weights) transform = tf.nn.bias_add(transform, biases) # reshape # 输出 Bx3x3 transform = tf.reshape(transform, [batch_size, 3, K]) return transform 针对64x64的网络设计与3x3的一样，只是改变了K值，对于文章提到让特征转化矩阵接近正交化，这样特征损失更小，这部分的实现是在分类任务对损失函数加入正则项，及添加权重reg_weight=0.001，下文分析中会标出。 2. 点云分类部分这部分主要分析训练代码和点云分类模型的设计 train.py 12345678910111213141516171819202122232425262728293031323334353637# 参数输入处理parser = argparse.ArgumentParser()parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')parser.add_argument('--model', default='pointnet_cls', help='Model name: pointnet_cls or pointnet_cls_basic [default: pointnet_cls]')parser.add_argument('--log_dir', default='log', help='Log dir [default: log]')parser.add_argument('--num_point', type=int, default=1024, help='Point Number [256/512/1024/2048] [default: 1024]')parser.add_argument('--max_epoch', type=int, default=250, help='Epoch to run [default: 250]')parser.add_argument('--batch_size', type=int, default=32, help='Batch Size during training [default: 32]')parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')parser.add_argument('--decay_step', type=int, default=200000, help='Decay step for lr decay [default: 200000]')parser.add_argument('--decay_rate', type=float, default=0.7, help='Decay rate for lr decay [default: 0.8]')FLAGS = parser.parse_args()BATCH_SIZE = FLAGS.batch_size # 训练批次大小NUM_POINT = FLAGS.num_point # 训练点云点个数MAX_EPOCH = FLAGS.max_epoch # 最大训练次数BASE_LEARNING_RATE = FLAGS.learning_rate # 初始学习率GPU_INDEX = FLAGS.gpu # 默认GPU使用数量MOMENTUM = FLAGS.momentum # 初始学习率OPTIMIZER = FLAGS.optimizer # 优化器DECAY_STEP = FLAGS.decay_step # 衰变步长DECAY_RATE = FLAGS.decay_rate # 衰变率# some code ...# 获取模型pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay) 原始点云nx3与T-Net训练后得到的3x3旋转矩阵相乘后，可以理解为变换为一组新的坐标下的点云数据。 models/pointnet_cls.py中的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104# 1.原始点云与3x3的T变换矩阵with tf.variable_scope('transform_net1') as sc: transform = input_transform_net(point_cloud, is_training, bn_decay, K=3)point_cloud_transformed = tf.matmul(point_cloud, transform)input_image = tf.expand_dims(point_cloud_transformed, -1)# 2.mlp(64,64)：使用2次卷积# 输入 Bxnx3x1# 64个1x3卷积核# 输出 Bxnx1x64net = tf_util.conv2d(input_image, 64, [1,3], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='conv1', bn_decay=bn_decay)# 输入 Bxnx1x64# 输出 Bxnx1x64net = tf_util.conv2d(net, 64, [1,1], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='conv2', bn_decay=bn_decay)# 3.接64x64特征转换矩阵with tf.variable_scope('transform_net2') as sc: transform = feature_transform_net(net, is_training, bn_decay, K=64)end_points['transform'] = transform# 将上一步的net Bxnx1x64压缩为 Bxnx64 和 T-Net的Bx64x64 相乘net_transformed = tf.matmul(tf.squeeze(net, axis=[2]), transform)# 4.接3次卷积和1次池化，对应图中的mlp(64,128,1024)+maxpoolpointnet_cls.py# Bxnx64 扩展为 Bxnx1x64net_transformed = tf.expand_dims(net_transformed, [2])# 输入 Bxnx1x64# 使用64个1x1卷积核# 输出 Bxnx1x64net = tf_util.conv2d(net_transformed, 64, [1,1], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='conv3', bn_decay=bn_decay)# 输入 Bxnx1x64# 使用128个1x1卷积核# 输出 Bxnx1x128net = tf_util.conv2d(net, 128, [1,1], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='conv4', bn_decay=bn_decay)# 输入 Bxnx1x128# 使用1024个1x1卷积核# 输出 Bxnx1x1024net = tf_util.conv2d(net, 1024, [1,1], padding='VALID', stride=[1,1], bn=True, is_training=is_training, scope='conv5', bn_decay=bn_decay)# Symmetric function: max pooling# 输入 Bxnx1x1024# 输出 Bx1x1x1024net = tf_util.max_pool2d(net, [num_point,1], padding='VALID', scope='maxpool')# 输出 Bx1024net = tf.reshape(net, [batch_size, -1]) # 5.全连接1 + dropout# 输出 batch_size x 512net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training, scope='dp1')# 全连接2 + dropout# 输出 batch_size x 256net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training, scope='fc2', bn_decay=bn_decay)net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training, scope='dp2')# 全连接3# 输出 batch_size x 40,因为ModelNet数据集有40个类别的模型net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3')return net, end_points# 6.使用交叉熵损失函数计算loss,对网络结构进行训练。def get_loss(pred, label, end_points, reg_weight=0.001): \"\"\" 预测值pred: B*NUM_CLASSES, 标签值label: B, \"\"\" loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label) classify_loss = tf.reduce_mean(loss) tf.summary.scalar('classify loss', classify_loss) # Enforce the transformation as orthogonal matrix transform = end_points['transform'] # BxKxK K = transform.get_shape()[1].value mat_diff = tf.matmul(transform, tf.transpose(transform, perm=[0,2,1])) mat_diff -= tf.constant(np.eye(K), dtype=tf.float32) mat_diff_loss = tf.nn.l2_loss(mat_diff) tf.summary.scalar('mat loss', mat_diff_loss) #这里加入了reg_weight正则项，使得T-net部分获得的特征转移矩阵更接近与正交化 return classify_loss + mat_diff_loss * reg_weight# 模型优化器选择：adam# 学习率初始值:0.001# 动量=0.9# batch_size =32# 学习率每训练20个epochs后减半# GTX1080 ModelNet 需要3-6小时 对于pointnet_cls_basic.py没有使用T-net的点云分类，网络结构更容易理解，比加入T-net的结构性能略低。 3. 点云分割部分 点之间的相关性问题 针对与分割物体上的问题与分类任务不同，分类任务中特征经过max pooling得到一维特征向量，它包含了全局信息，再经过全连接网络，得到1*K的k个类别预测得分即为分类结果。而分割任务中，需要对每一个点输出所属类别，使用类似二维图像分割的上采样过程（跳步连接skip-links）。Pointnet针对分割任务也使用了类似图像分割任务的，高层全局信息与底层局部特征结合的思想。 针对Pointnet论文作者提供的版本（Tensorflow）的源码如下：https://github.com/charlesq34/pointnet 对于pointnet源码其余部分的介绍不详细展开，根据个人理解将源码的结构与功能设计展示如下： 分割部分的代码实现主要在part_seg/（部件分割）和sem_seg/（场景分割）下。其中part_seg中底层局部特征与高层全局特征的连接（concat）使用到了各层特征。 sem_seg/model.py场景分割中需要注意论文使用的S3DIS数据维度不再是3维而是更高的9维度（XYZ+RGB+相对于房间的标准化后的位置信息），针对特征连接部分使用高层全局特征（B*1024）接全连接降维到128，然后与高维特征自身做concat，不是采用论文中提到的方式。 而论文中提到的分割结构实际是在models/pointnet_seg.py中实现，即max pooling后的1D特征向量，使用tf.tile()复制n份（n个特征点），与之前网络得到的 n * 64特征矩阵分别concat。得到一个n(64+D)的特征矩阵，再经一系列的特征变换操作，得到每个点的分类结果。 结语本文主要结合代码层面总结了pointnet网络的分类和分割任务的实现。主要是理解pointnet是如何做到直接从原始点云数据中提取高维特征，并且解决好点云的特性。实际上基于pointnet结构可以进行很多任务，比如点云配准，物体检测，3D重建，法向量估计等，只需要根据具体任务合理修改网络后几层的结构，利用好网络提取的高维特征。 针对pointnet存在的点与点之间相关性的缺失，在pointnet++中使用局部采样+分组+pointnet的结构进行解决，并考虑到了点云的稀疏性解决方案，之后很多深度学习的研究在此基础上展开，习惯上称为pointnet家族（point-wise MLP），比如Frustum,flowNet 3D,LSAnet,PAT等等。个人认为更高的准确度需要点云等3D数据与图像结合进行深度学习训练，将图像的高分辨率优势借鉴进来会有更好的效果。 源码地址：1.原论文实现代码https://github.com/charlesq34/pointnet2.基于pytorch实现：https://github.com/fxia22/pointnet.pytorchhttps://github.com/yanx27/Pointnet_Pointnet2_pytorch","link":"","categories":[{"name":"Pointcloud,DL","slug":"Pointcloud-DL","permalink":"https://Yansz.github.io/categories/Pointcloud-DL/"}],"tags":[]},{"title":"About point cloud analysis","date":"2020-01-09T21:00:00.000Z","path":"2020/01/10/Point cloud analysis /","text":"Since 2017Sort out papers on point cloud research from 2017, and add keywords for the research area. For anyone who wants to do research about 3D point cloud, thanks for the Open source community .If you find the awesome paper/code/dataset or have some suggestions, please contact linhua2017@ia.ac.cn. Keywords dat.: dataset &emsp; | &emsp; cls.: classification &emsp;| &emsp; rel.: retrieval &emsp;|&emsp; seg.: segmentation det.: detection &emsp; | &emsp; tra.: tracking &emsp; | &emsp; pos.: pose &emsp; | &emsp; dep.: depth reg.: registration &emsp;| &emsp; rec.: reconstruction &emsp; | &emsp; aut.: autonomous driving oth.: other, including normal-related, correspondence, mapping, matching, alignment, compression, generative model… Statistics: :fire:code is available &amp; stars &gt;= 100 &emsp;|&emsp; :star: citation &gt;= 50 2017 [CVPR] PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. [tensorflow][pytorch] [cls. seg. det.] :fire: :star: [CVPR] Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs. [cls.] :star: [CVPR] SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation. [torch] [seg. oth.] :star: [CVPR] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. [project][git] [dat. cls. rel. seg. oth.] :fire: :star: [CVPR] Scalable Surface Reconstruction from Point Clouds with Extreme Scale and Density Diversity. [oth.] [CVPR] Efficient Global Point Cloud Alignment using Bayesian Nonparametric Mixtures. [code] [oth.] [CVPR] Discriminative Optimization: Theory and Applications to Point Cloud Registration. [reg.] [CVPR] 3D Point Cloud Registration for Localization using a Deep Neural Network Auto-Encoder. [git] [reg.] [CVPR] Multi-View 3D Object Detection Network for Autonomous Driving. [tensorflow] [det. aut.] :fire: :star: [CVPR] 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions. [code] [dat. pos. reg. rec. oth.] :fire: :star: [CVPR] OctNet: Learning Deep 3D Representations at High Resolutions. [torch] [cls. seg. oth.] :fire: :star: [ICCV] Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models. [pytorch] [cls. rel. seg.] :star: [ICCV] 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds. [code] [seg.] [ICCV] Colored Point Cloud Registration Revisited. [reg.] [ICCV] PolyFit: Polygonal Surface Reconstruction from Point Clouds. [code] [rec.] :fire: [ICCV] From Point Clouds to Mesh using Regression. [rec.] [ICCV] 3D Graph Neural Networks for RGBD Semantic Segmentation. [pytorch] [seg.] [NeurIPS] PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. [tensorflow][pytorch] [cls. seg.] :fire: :star: [NeurIPS] Deep Sets. [pytorch] [cls.] :star: [ICRA] Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks. [code] [det. aut.] :star: [ICRA] Fast segmentation of 3D point clouds: A paradigm on LiDAR data for autonomous vehicle applications. [code] [seg. aut.] [ICRA] SegMatch: Segment based place recognition in 3D point clouds. [seg. oth.] [ICRA] Using 2 point+normal sets for fast registration of point clouds with small overlap. [reg.] [IROS] Car detection for autonomous vehicle: LIDAR and vision fusion approach through deep learning framework. [det. aut.] [IROS] 3D object classification with point convolution network. [cls.] [IROS] 3D fully convolutional network for vehicle detection in point cloud. [tensorflow] [det. aut.] :fire: :star: [IROS] Deep learning of directional truncated signed distance function for robust 3D object recognition. [det. pos.] [IROS] Analyzing the quality of matched 3D point clouds of objects. [oth.] [3DV] SEGCloud: Semantic Segmentation of 3D Point Clouds. [project] [seg. aut.] :star: [TPAMI] Structure-aware Data Consolidation. [oth.] 2018 [CVPR] SPLATNet: Sparse Lattice Networks for Point Cloud Processing. [caffe] [seg.] :fire: [CVPR] Attentional ShapeContextNet for Point Cloud Recognition. [cls. seg.] [CVPR] Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling. [code] [cls. seg.] [CVPR] FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation. [code] [cls.] [CVPR] Pointwise Convolutional Neural Networks. [tensorflow] [cls. seg.] [CVPR] PU-Net: Point Cloud Upsampling Network. [tensorflow] [rec. oth.] :fire: [CVPR] SO-Net: Self-Organizing Network for Point Cloud Analysis. [pytorch] [cls. seg.] :fire: :star: [CVPR] Recurrent Slice Networks for 3D Segmentation of Point Clouds. [pytorch] [seg.] [CVPR] 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks. [pytorch] [seg.] :fire: [CVPR] Deep Parametric Continuous Convolutional Neural Networks. [seg. aut.] [CVPR] PIXOR: Real-time 3D Object Detection from Point Clouds. [pytorch] [det. aut.] [CVPR] SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation. [tensorflow] [seg.] :fire: [CVPR] Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. [pytorch] [seg.] :fire: [CVPR] VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. [tensorflow] [det. aut.] :fire: :star: [CVPR] Reflection Removal for Large-Scale 3D Point Clouds. [oth.] [CVPR] Hand PointNet: 3D Hand Pose Estimation using Point Sets. [pytorch] [pos.] [CVPR] PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition. [tensorflow] [rel.] :fire: [CVPR] A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation. [cls.] [CVPR] Density Adaptive Point Set Registration. [code] [reg.] [CVPR] A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds. [seg.] [CVPR] Inverse Composition Discriminative Optimization for Point Cloud Registration. [reg.] [CVPR] CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles. [tra. det. rec.] [CVPR] PPFNet: Global Context Aware Local Features for Robust 3D Point Matching. [oth.] [CVPR] PointGrid: A Deep Network for 3D Shape Understanding. [tensorflow] [cls. seg.] [CVPR] PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation. [code] [det. aut.] [CVPR] Frustum PointNets for 3D Object Detection from RGB-D Data. [tensorflow] [det. aut.] :fire: :star: [CVPR] Tangent Convolutions for Dense Prediction in 3D. [tensorflow] [seg. aut.] [ECCV] Multiresolution Tree Networks for 3D Point Cloud Processing. [pytorch] [cls.] [ECCV] EC-Net: an Edge-aware Point set Consolidation Network. [tensorflow] [oth.] [ECCV] 3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation. [seg.] [ECCV] Learning and Matching Multi-View Descriptors for Registration of Point Clouds. [reg.] [ECCV] 3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration. [tensorflow] [reg.] [ECCV] Local Spectral Graph Convolution for Point Set Feature Learning. [tensorflow] [cls. seg.] [ECCV] SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters. [tensorflow] [cls. seg.] [ECCV] Efficient Global Point Cloud Registration by Matching Rotation Invariant Features Through Translation Search. [reg.] [ECCV] Efficient Dense Point Cloud Object Reconstruction using Deformation Vector Fields. [rec.] [ECCV] Fully-Convolutional Point Networks for Large-Scale Point Clouds. [tensorflow] [seg. oth.] [ECCV] Deep Continuous Fusion for Multi-Sensor 3D Object Detection. [det.] [ECCV] HGMR: Hierarchical Gaussian Mixtures for Adaptive 3D Registration. [reg.] [ECCV] Point-to-Point Regression PointNet for 3D Hand Pose Estimation. [pos.] [ECCV] PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors. [oth.] [ECCVW] 3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues. [cls. seg.] [ECCVW] YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud. [det. aut.] [AAAI] Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction. [tensorflow] [rec.] :fire: [AAAI] Adaptive Graph Convolutional Neural Networks. [cls.] [NeurIPS] Unsupervised Learning of Shape and Pose with Differentiable Point Clouds. [tensorflow] [pos.] [NeurIPS] PointCNN: Convolution On X-Transformed Points. [tensorflow][pytorch] [cls. seg.] :fire: [ICML] Learning Representations and Generative Models for 3D Point Clouds. [code] [oth.] :fire: [TOG] Point Convolutional Neural Networks by Extension Operators. [tensorflow] [cls. seg.] [SIGGRAPH] P2P-NET: Bidirectional Point Displacement Net for Shape Transform. [tensorflow] [oth.] [SIGGRAPH Asia] Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds. [tensorflow] [cls. seg. oth.] [SIGGRAPH] Learning local shape descriptors from part correspondences with multi-view convolutional networks. [project] [seg. oth.] [MM] PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition. [cls. rel.] [MM] RGCNN: Regularized Graph CNN for Point Cloud Segmentation. [tensorflow] [seg.] [MM] Hybrid Point Cloud Attribute Compression Using Slice-based Layered Structure and Block-based Intra Prediction. [oth.] [ICRA] End-to-end Learning of Multi-sensor 3D Tracking by Detection. [det. tra. aut.] [ICRA] Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping. [seg. oth.] [ICRA] SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud. [tensorflow] [seg. aut.] [ICRA] Robust Real-Time 3D Person Detection for Indoor and Outdoor Applications. [det.] [ICRA] High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion. [dep. aut.] [ICRA] Sampled-Point Network for Classification of Deformed Building Element Point Clouds. [cls.] [ICRA] Gemsketch: Interactive Image-Guided Geometry Extraction from Point Clouds. [oth.] [ICRA] Signature of Topologically Persistent Points for 3D Point Cloud Description. [oth.] [ICRA] A General Pipeline for 3D Detection of Vehicles. [det. aut.] [ICRA] Robust and Fast 3D Scan Alignment Using Mutual Information. [oth.] [ICRA] Delight: An Efficient Descriptor for Global Localisation Using LiDAR Intensities. [oth.] [ICRA] Surface-Based Exploration for Autonomous 3D Modeling. [oth. aut.] [ICRA] Deep Lidar CNN to Understand the Dynamics of Moving Vehicles. [oth. aut.] [ICRA] Dex-Net 3.0: Computing Robust Vacuum Suction Grasp Targets in Point Clouds Using a New Analytic Model and Deep Learning. [oth.] [ICRA] Real-Time Object Tracking in Sparse Point Clouds Based on 3D Interpolation. [tra.] [ICRA] Robust Generalized Point Cloud Registration Using Hybrid Mixture Model. [reg.] [ICRA] A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration. [reg.] [ICRA] Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping. [oth.] [ICRA] Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System. [oth.] [ICRA] Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data. [cls.] [ICRA] Asynchronous Multi-Sensor Fusion for 3D Mapping and Localization. [oth.] [ICRA] Complex Urban LiDAR Data Set. [video] [dat. oth.] [IROS] CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.[tensorflow] [oth. aut.] [IROS] Dynamic Scaling Factors of Covariances for Accurate 3D Normal Distributions Transform Registration. [reg.] [IROS] A 3D Laparoscopic Imaging System Based on Stereo-Photogrammetry with Random Patterns. [rec. oth.] [IROS] Robust Generalized Point Cloud Registration with Expectation Maximization Considering Anisotropic Positional Uncertainties. [reg.] [IROS] Octree map based on sparse point cloud and heuristic probability distribution for labeled images. [oth. aut.] [IROS] PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization. [oth.] [IROS] Scan Context: Egocentric Spatial Descriptor for Place Recognition Within 3D Point Cloud Map. [oth.] [IROS] LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain.[code] [pos. oth.] :fire: [IROS] Classification of Hanging Garments Using Learned Features Extracted from 3D Point Clouds. [cls.] [IROS] Stereo Camera Localization in 3D LiDAR Maps. [pos. oth.] [IROS] Joint 3D Proposal Generation and Object Detection from View Aggregation. [det.] :star: [IROS] Joint Point Cloud and Image Based Localization for Efficient Inspection in Mixed Reality. [oth.] [IROS] Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding. [det. oth.] [IROS] NDVI Point Cloud Generator Tool Using Low-Cost RGB-D Sensor. [code][oth.] [IROS] A 3D Convolutional Neural Network Towards Real-Time Amodal 3D Object Detection. [det. pos.] [IROS] Extracting Phenotypic Characteristics of Corn Crops Through the Use of Reconstructed 3D Models. [seg. rec.] [IROS] PCAOT: A Manhattan Point Cloud Registration Method Towards Large Rotation and Small Overlap. [reg.] [IROS] [Tensorflow]3DmFV: Point Cloud Classification and segmentation for unstructured 3D point clouds. [cls. ] [IROS] Seeing the Wood for the Trees: Reliable Localization in Urban and Natural Environments. [oth. ] [SENSORS] SECOND: Sparsely Embedded Convolutional Detection. [pytorch] [det. aut.] :fire: [ACCV] Flex-Convolution (Million-Scale Point-Cloud Learning Beyond Grid-Worlds). [tensorflow] [seg.] [3DV] PCN: Point Completion Network. [tensorflow] [reg. oth. aut.] :fire: [ICASSP] A Graph-CNN for 3D Point Cloud Classification. [tensorflow] [cls.] :fire: [ITSC] BirdNet: a 3D Object Detection Framework from LiDAR information. [det. aut.] [arXiv] PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation. [tensorflow] [seg.] :fire: [arXiv] Spherical Convolutional Neural Network for 3D Point Clouds. [cls.] [arXiv] Adversarial Autoencoders for Generating 3D Point Clouds. [oth.] [arXiv] Iterative Transformer Network for 3D Point Cloud. [cls. seg. pos.] [arXiv] Topology-Aware Surface Reconstruction for Point Clouds. [rec.] [arXiv] Inferring Point Clouds from Single Monocular Images by Depth Intermediation. [oth.] [arXiv] Deep RBFNet: Point Cloud Feature Learning using Radial Basis Functions. [cls.] [arXiv] IPOD: Intensive Point-based Object Detector for Point Cloud. [det.] [arXiv] Feature Preserving and Uniformity-controllable Point Cloud Simplification on Graph. [oth.] [arXiv] POINTCLEANNET: Learning to Denoise and Remove Outliers from Dense Point Clouds. [pytorch] [oth.] [arXiv] Complex-YOLO: Real-time 3D Object Detection on Point Clouds. [pytorch] [det. aut.] :fire: [arxiv] RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement. [tensorflow] [det. aut.] [arXiv] Multi-column Point-CNN for Sketch Segmentation. [seg.] [arXiv] PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention. [project] [oth.] [arXiv] Point Cloud GAN. [pytorch] [oth.] 2019 [CVPR] Relation-Shape Convolutional Neural Network for Point Cloud Analysis. [pytorch] [cls. seg. oth.] :fire: [CVPR] Spherical Fractal Convolutional Neural Networks for Point Cloud Recognition. [cls. seg.] [CVPR] DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds. [code] [reg.] [CVPR] Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving. [code] [det. dep. aut.] [CVPR] PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud. [pytorch] [det. aut.] :fire: [CVPR] Generating 3D Adversarial Point Clouds. [code] [oth.] [CVPR] Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling. [cls. seg.] [CVPR] A-CNN: Annularly Convolutional Neural Networks on Point Clouds. [tensorflow][cls. seg.] [CVPR] PointConv: Deep Convolutional Networks on 3D Point Clouds. [tensorflow] [cls. seg.] :fire: [CVPR] Path-Invariant Map Networks. [tensorflow] [seg. oth.] [CVPR] PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding. [code] [dat. seg.] [CVPR] GeoNet: Deep Geodesic Networks for Point Cloud Analysis. [cls. rec. oth.] [CVPR] Associatively Segmenting Instances and Semantics in Point Clouds. [tensorflow] [seg.] :fire: [CVPR] Supervised Fitting of Geometric Primitives to 3D Point Clouds. [tensorflow] [oth.] [CVPR] Octree guided CNN with Spherical Kernels for 3D Point Clouds. [extension] [code] [cls. seg.] [CVPR] PointNetLK: Point Cloud Registration using PointNet. [pytorch] [reg.] [CVPR] JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields. [pytorch] [seg.] [CVPR] Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning. [seg.] [CVPR] PointPillars: Fast Encoders for Object Detection from Point Clouds. [pytorch] [det.] :fire: [CVPR] Patch-based Progressive 3D Point Set Upsampling. [tensorflow] [oth.] [CVPR] PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval. [code] [rel.] [CVPR] PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation. [pytorch] [dat. seg.] [CVPR] PointFlowNet: Learning Representations for Rigid Motion Estimation from Point Clouds. [code] [det. dat. oth.] [CVPR] SDRSAC: Semidefinite-Based Randomized Approach for Robust Point Cloud Registration without Correspondences. [matlab] [reg.] [CVPR] Deep Reinforcement Learning of Volume-guided Progressive View Inpainting for 3D Point Scene Completion from a Single Depth Image. [rec. oth.] [CVPR] Embodied Question Answering in Photorealistic Environments with Point Cloud Perception. [oth.] [CVPR] 3D Point-Capsule Networks. [pytorch] [cls. rec. oth.] [CVPR] 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. [pytorch] [seg.] :fire: [CVPR] The Perfect Match: 3D Point Cloud Matching with Smoothed Densities. [tensorflow] [oth.] [CVPR] FilterReg: Robust and Efficient Probabilistic Point-Set Registration using Gaussian Filter and Twist Parameterization. [code] [reg.] [CVPR] FlowNet3D: Learning Scene Flow in 3D Point Clouds. [oth.] [CVPR] Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN. [cls. det.] [CVPR] ClusterNet: Deep Hierarchical Cluster Network with Rigorously Rotation-Invariant Representation for Point Cloud Analysis. [cls.] [CVPR] PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. [pytorch] [cls. seg.] [CVPR] RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion. [code] [oth.] [CVPR] PointNetLK: Robust &amp; Efficient Point Cloud Registration using PointNet. [pytorch] [reg.] [CVPR] Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes. [code] [rec.] [CVPR] Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds using Convolutional Neural Networks. [tensorflow] [oth.] [CVPR] GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud. [seg.] [CVPR] Graph Attention Convolution for Point Cloud Semantic Segmentation. [seg.] [CVPR] Point-to-Pose Voting based Hand Pose Estimation using Residual Permutation Equivariant Layer. [pos.] [CVPR] LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving. [det. aut.] [CVPR] LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks. [project] [cls. seg.] [CVPR] Structural Relational Reasoning of Point Clouds. [cls. seg.] [CVPR] 3DN: 3D Deformation Network. [tensorflow] [rec. oth.] [CVPR] Privacy Preserving Image-Based Localization. [pos. oth.] [CVPR] Argoverse: 3D Tracking and Forecasting With Rich Maps.[tra. aut.] [CVPR] Leveraging Shape Completion for 3D Siamese Tracking. [pytorch] [tra. ] [CVPRW] Attentional PointNet for 3D-Object Detection in Point Clouds. [pytorch] [cls. det. aut.] [CVPR] 3D Local Features for Direct Pairwise Registration. [reg.] [CVPR] Learning to Sample. [tensorflow] [cls. rec.] [CVPR] Revealing Scenes by Inverting Structure from Motion Reconstructions. [code] [rec.] [CVPR] DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image. [pytorch] [dep.] [CVPR] HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds. [pytorch] [oth.] [ICCV] Deep Hough Voting for 3D Object Detection in Point Clouds. [pytorch] [tensorflow] [det.] :fire: [ICCV] DeepGCNs: Can GCNs Go as Deep as CNNs? [tensorflow] [pytorch] [seg.] :fire: [ICCV] PU-GAN: a Point Cloud Upsampling Adversarial Network. [tensorflow] [oth.] [ICCV] 3D Point Cloud Learning for Large-scale Environment Analysis and Place Recognition. [rel. oth.] [ICCV] PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows. [pytorch] [oth.] [ICCV] Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction. [oth.] [ICCV] SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation with Semi-supervised Learning. [code] [pos.] [ICCV] DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense. [oth.] [ICCV] Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data. [cls. dat.] [code] [dataset] [ICCV] KPConv: Flexible and Deformable Convolution for Point Clouds. [tensorflow] [cls. seg.] :fire: [ICCV] ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics. [project] [seg.] [ICCV] Point-Based Multi-View Stereo Network. [pytorch] [rec.] [ICCV] DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing. [pytorch] [cls. seg. oth.] [ICCV] DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration. [reg.] [ICCV] 3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions. [pytorch] [oth.] [ICCV] Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation. [seg.] [ICCV] Learning an Effective Equivariant 3D Descriptor Without Supervision. [oth.] [ICCV] Fully Convolutional Geometric Features. [pytorch] [reg.] [ICCV] LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis. [oth. aut.] [ICCV] Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning. [tensorflow] [oth.] [ICCV] USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds. [pytorch] [oth.] [ICCV] Interpolated Convolutional Networks for 3D Point Cloud Understanding. [cls. seg.] [ICCV] PointCloud Saliency Maps. [code] [oth.] [ICCV] STD: Sparse-to-Dense 3D Object Detector for Point Cloud. [det. oth.] [ICCV] Accelerated Gravitational Point Set Alignment with Altered Physical Laws. [reg.] [ICCV] Deep Closest Point: Learning Representations for Point Cloud Registration. [reg.] [ICCV] Efficient Learning on Point Clouds with Basis Point Sets. [code] [cls. reg.] [ICCV] PointAE: Point Auto-encoder for 3D Statistical Shape and Texture Modelling. [rec.] [ICCV] Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds. [rec.] [ICCV] Dynamic Points Agglomeration for Hierarchical Point Sets Learning. [pytorch] [cls. seg.] [ICCV] Unsupervised Multi-Task Feature Learning on Point Clouds. [cls. seg.] [ICCV] VV-NET: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation. [tensorflow] [seg.] [ICCV] GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion. [pytorch] [rec.] [ICCV] MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences. [code] [cls. seg. oth.] [ICCV] Fast Point R-CNN. [det. aut.] [ICCV] Robust Variational Bayesian Point Set Registration. [reg.] [ICCV] DiscoNet: Shapes Learning on Disconnected Manifolds for 3D Editing. [rec. oth.] [ICCV] Learning an Effective Equivariant 3D Descriptor Without Supervision. [oth.] [ICCV] 3D Instance Segmentation via Multi-Task Metric Learning. [code] [seg.] [ICCV] 3D Face Modeling From Diverse Raw Scan Data. [rec.] [ICCVW] Range Adaptation for 3D Object Detection in LiDAR. [det. aut.] [NeurIPS] Self-Supervised Deep Learning on Point Clouds by Reconstructing Space. [cls. oth.] [NeurIPS] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [tensorflow] [det. seg.] [NeurIPS] Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations. [tensorflow] [seg.] [NeurIPS] Point-Voxel CNN for Efficient 3D Deep Learning. [det. seg. aut.] [NeurIPS] PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation. [code] [cls. oth.] [ICLR] Learning Localized Generative Models for 3D Point Clouds via Graph Convolution. [oth.] [ICMLW] LiDAR Sensor modeling and Data augmentation with GANs for Autonomous driving. [det. oth. aut.] [AAAI] CAPNet: Continuous Approximation Projection For 3D Point Cloud Reconstruction Using 2D Supervision. [code] [rec.] [AAAI] Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network. [tensorflow] [cls. seg.] [AAAI] Point Cloud Processing via Recurrent Set Encoding. [cls.] [AAAI] PVRNet: Point-View Relation Neural Network for 3D Shape Recognition. [pytorch] [cls. rel.] [AAAI] Hypergraph Neural Networks. [pytorch] [cls.] [TOG] Dynamic Graph CNN for Learning on Point Clouds. [tensorflow][pytorch] [cls. seg.] :fire: :star: [TOG] LOGAN: Unpaired Shape Transform in Latent Overcomplete Space. [tensorflow] [oth.] [SIGGRAPH Asia] StructureNet: Hierarchical Graph Networks for 3D Shape Generation. [seg. oth.] [MM] MMJN: Multi-Modal Joint Networks for 3D Shape Recognition. [cls. rel.] [MM] 3D Point Cloud Geometry Compression on Deep Learning. [oth.] [MM] SRINet: Learning Strictly Rotation-Invariant Representations for Point Cloud Classification and Segmentation. [tensorflow] [cls. seg.] [MM] L2G Auto-encoder: Understanding Point Clouds by Local-to-Global Reconstruction with Hierarchical Self-Attention. [cls. rel.] [MM] Ground-Aware Point Cloud Semantic Segmentation for Autonomous Driving. [code] [seg. aut.] [ICME] Justlookup: One Millisecond Deep Feature Extraction for Point Clouds By Lookup Tables. [cls. rel.] [ICASSP] 3D Point Cloud Denoising via Deep Neural Network based Local Surface Estimation. [code] [oth.] [BMVC] Mitigating the Hubness Problem for Zero-Shot Learning of 3D Objects. [cls.] [ICRA] Discrete Rotation Equivariance for Point Cloud Recognition. [pytorch] [cls.] [ICRA] SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud. [tensorflow] [seg. aut.] [ICRA] Detection and Tracking of Small Objects in Sparse 3D Laser Range Data. [det. tra. aut.] [ICRA] Oriented Point Sampling for Plane Detection in Unorganized Point Clouds. [det. seg.] [ICRA] Point Cloud Compression for 3D LiDAR Sensor Using Recurrent Neural Network with Residual Blocks. [pytorch] [oth.] [ICRA] Focal Loss in 3D Object Detection. [code] [det. aut.] [ICRA] PointNetGPD: Detecting Grasp Configurations from Point Sets. [pytorch] [det. seg.] [ICRA] 2D3D-MatchNet: Learning to Match Keypoints across 2D Image and 3D Point Cloud. [oth.] [ICRA] Speeding up Iterative Closest Point Using Stochastic Gradient Descent. [oth.] [ICRA] Uncertainty Estimation for Projecting Lidar Points Onto Camera Images for Moving Platforms. [oth.] [ICRA] SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data. [det. aut.] [ICRA] BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving. [project] [dat. det. tra. aut. oth.] [ICRA] A Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications. [det.] [ICRA] Robust low-overlap 3-D point cloud registration for outlier rejection. [matlab] [reg.] [ICRA] Robust 3D Object Classification by Combining Point Pair Features and Graph Convolution. [cls. seg.] [ICRA] Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds. [seg.] [ICRA] Robust Generalized Point Set Registration Using Inhomogeneous Hybrid Mixture Models Via Expectation. [reg.] [ICRA] Dense 3D Visual Mapping via Semantic Simplification. [oth.] [ICRA] MVX-Net: Multimodal VoxelNet for 3D Object Detection. [det. aut.] [ICRA] CELLO-3D: Estimating the Covariance of ICP in the Real World. [reg.] [IROS] EPN: Edge-Aware PointNet for Object Recognition from Multi-View 2.5D Point Clouds. [tensorflow] [cls. det.] [IROS] SeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on Large-Scale Point Cloud Description for Self-Driving Vehicles. [oth.] [aut.] [IROS] PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud. [seg. aut.] [IV] End-to-End 3D-PointCloud Semantic Segmentation for Autonomous Driving. [seg.] [aut.] [Eurographics Workshop] Generalizing Discrete Convolutions for Unstructured Point Clouds. [pytorch] [cls. seg.] [WACV] 3DCapsule: Extending the Capsule Architecture to Classify 3D Point Clouds. [cls.] [3DV] Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. [project] [cls. seg.] [3DV] Effective Rotation-invariant Point CNN with Spherical Harmonics kernels. [tensorflow] [cls. seg. oth.] [TVCG] LassoNet: Deep Lasso-Selection of 3D Point Clouds. [project] [oth.] [arXiv] Fast 3D Line Segment Detection From Unorganized Point Cloud. [det.] [arXiv] Point-Cloud Saliency Maps. [tensorflow] [cls. oth.] [arXiv] Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud Classifiers. [code] [oth.] [arxiv] Context Prediction for Unsupervised Deep Learning on Point Clouds. [cls. seg.] [arXiv] Points2Pix: 3D Point-Cloud to Image Translation using conditional Generative Adversarial Networks. [oth.] [arXiv] NeuralSampler: Euclidean Point Cloud Auto-Encoder and Sampler. [cls. oth.] [arXiv] 3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation. [seg.] [arXiv] Zero-shot Learning of 3D Point Cloud Objects. [code] [cls.] [arXiv] Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud. [det. aut.] [arXiv] Real-time Multiple People Hand Localization in 4D Point Clouds. [det. oth.] [arXiv] Variational Graph Methods for Efficient Point Cloud Sparsification. [oth.] [arXiv] Neural Style Transfer for Point Clouds. [oth.] [arXiv] OREOS: Oriented Recognition of 3D Point Clouds in Outdoor Scenarios. [pos. oth.] [arXiv] FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds. [code] [det. aut.] [arXiv] Unpaired Point Cloud Completion on Real Scans using Adversarial Training. [oth.] [arXiv] MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds. [cls. seg.] [arXiv] DeepPoint3D: Learning Discriminative Local Descriptors using Deep Metric Learning on 3D Point Clouds. [cls. rel. oth.] [arXiv] Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds. [pytorch] [det. tra. aut.] :fire: [arXiv] Graph-based Inpainting for 3D Dynamic Point Clouds. [oth.] [arXiv] nuScenes: A multimodal dataset for autonomous driving. [link] [dat. det. tra. aut.] [arXiv] 3D Backbone Network for 3D Object Detection. [code] [det. aut.] [arXiv] Adversarial Autoencoders for Compact Representations of 3D Point Clouds. [pytorch] [rel. oth.] [arXiv] Linked Dynamic Graph CNN: Learning on Point Cloud via Linking Hierarchical Features. [cls. seg.] [arXiv] GAPNet: Graph Attention based Point Neural Network for Exploiting Local Feature of Point Cloud. [tensorflow] [cls. seg.] [arXiv] Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. [tensorflow] [det. seg.] [arXiv] Differentiable Surface Splatting for Point-based Geometry Processing. [pytorch] [oth.] [arXiv] Spatial Transformer for 3D Points. [seg.] [arXiv] Point-Voxel CNN for Efficient 3D Deep Learning. [seg. det. aut.] [arXiv] Attentive Context Normalization for Robust Permutation-Equivariant Learning. [cls.] [arXiv] Neural Point-Based Graphics. [project] [oth.] [arXiv] Point Cloud Super Resolution with Adversarial Residual Graph Networks. [oth.] [tensorflow] [arXiv] Blended Convolution and Synthesis for Efficient Discrimination of 3D Shapes. [cls. rel.] [arXiv] StarNet: Targeted Computation for Object Detection in Point Clouds. [tensorflow] [det.] [arXiv] Efficient Tracking Proposals using 2D-3D Siamese Networks on LIDAR. [tra.] [arXiv] SAWNet: A Spatially Aware Deep Neural Network for 3D Point Cloud Processing. [tensorflow] [cls. seg.] [arXiv] Part-A^2 Net: 3D Part-Aware and Aggregation Neural Network for Object Detection from Point Cloud. [det. aut.] [arXiv] PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding Module for Classification and Segmentation. [cls. seg.] [arXiv] PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing. [tensorflow] [tra. oth. aut.] [arXiv] PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points. [tensorflow] [cls. seg.] [arXiv] Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds. [oth.] [arXiv] 3D-Rotation-Equivariant Quaternion Neural Networks. [cls. rec.] [arXiv] Point2SpatialCapsule: Aggregating Features and Spatial Relationships of Local Regions on Point Clouds using Spatial-aware Capsules. [cls. rel. seg.] [arXiv] RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds. [tensorflow] [seg.] [arXiv] Geometric Feedback Network for Point Cloud Classification. [cls.] [arXiv] SGAS: Sequential Greedy Architecture Search. [project] [code] [cls.] [arXiv] Relation Graph Network for 3D Object Detection in Point Clouds. [det.] [arXiv] Deformable Filter Convolution for Point Cloud Reasoning. [seg. det. aut.] [arXiv] PU-GCN: Point Cloud Upsampling via Graph Convolutional Network. [project] [oth.] [arXiv] StructEdit: Learning Structural Shape Variations. [project] [rec.] [arXiv] Grid-GCN for Fast and Scalable Point Cloud Learning. [seg. cls.] [arXiv] PointPainting: Sequential Fusion for 3D Object Detection. [seg. det.] [arXiv] Transductive Zero-Shot Learning for 3D Point Cloud Classification. [cls.] [arXiv] Geometry Sharing Network for 3D Point Cloud Classification and Segmentation. [pytorch] [cls. seg.] [arvix] Deep Learning for 3D Point Clouds: A Survey. [code] [cls. det. tra. seg.] [arXiv] Spectral-GANs for High-Resolution 3D Point-cloud Generation. [rec. oth.] [arXiv] Point Attention Network for Semantic Segmentation of 3D Point Clouds. [seg.] [arXiv] PLIN: A Network for Pseudo-LiDAR Point Cloud Interpolation. [oth.] 2020 [AAAI] Morphing and Sampling Network for Dense Point Cloud Completion. [pytorch] [oth.] [AAAI] TANet: Robust 3D Object Detection from Point Clouds with Triple Attention. [code] [det. aut.] [AAAI] Point2Node: Correlation Learning of Dynamic-Node for Point Cloud Feature Modeling. [seg. cls.] [AAAI] PRIN: Pointwise Rotation-Invariant Network. [seg. cls.] [WACV] FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data. [seg. aut.] 1Datasets [KITTI] The KITTI Vision Benchmark Suite. [det.] [ModelNet] The Princeton ModelNet . [cls.] [ShapeNet] A collaborative dataset between researchers at Princeton, Stanford and TTIC. [seg.] [PartNet] The PartNet dataset provides fine grained part annotation of objects in ShapeNetCore. [seg.] [PartNet] PartNet benchmark from Nanjing University and National University of Defense Technology. [seg.] [S3DIS] The Stanford Large-Scale 3D Indoor Spaces Dataset. [seg.] [ScanNet] Richly-annotated 3D Reconstructions of Indoor Scenes. [cls. seg.] [Stanford 3D] The Stanford 3D Scanning Repository. [reg.] [UWA Dataset] . [cls. seg. reg.] [Princeton Shape Benchmark] The Princeton Shape Benchmark. [SYDNEY URBAN OBJECTS DATASET] This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees. [cls. match.] [ASL Datasets Repository(ETH)] This site is dedicated to provide datasets for the Robotics community with the aim to facilitate result evaluations and comparisons. [cls. match. reg. det] [Large-Scale Point Cloud Classification Benchmark(ETH)] This benchmark closes the gap and provides a large labelled 3D point cloud data set of natural scenes with over 4 billion points in total. [cls.] [Robotic 3D Scan Repository] The Canadian Planetary Emulation Terrain 3D Mapping Dataset is a collection of three-dimensional laser scans gathered at two unique planetary analogue rover test facilities in Canada. [Radish] The Robotics Data Set Repository (Radish for short) provides a collection of standard robotics data sets. [IQmulus &amp; TerraMobilita Contest] The database contains 3D MLS data from a dense urban environment in Paris (France), composed of 300 million points. The acquisition was made in January 2013. [cls. seg. det.] [Oakland 3-D Point Cloud Dataset] This repository contains labeled 3-D point cloud laser data collected from a moving platform in a urban environment. [Robotic 3D Scan Repository] This repository provides 3D point clouds from robotic experiments，log files of robot runs and standard 3D data sets for the robotics community. [Ford Campus Vision and Lidar Data Set] The dataset is collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. [The Stanford Track Collection] This dataset contains about 14,000 labeled tracks of objects as observed in natural street scenes by a Velodyne HDL-64E S2 LIDAR. [PASCAL3D+] Beyond PASCAL: A Benchmark for 3D Object Detection in the Wild. [pos. det.] [3D MNIST] The aim of this dataset is to provide a simple way to get started with 3D computer vision problems such as 3D shape recognition. [cls.] [WAD] [ApolloScape] The datasets are provided by Baidu Inc. [tra. seg. det.] [nuScenes] The nuScenes dataset is a large-scale autonomous driving dataset. [PreSIL] Depth information, semantic segmentation (images), point-wise segmentation (point clouds), ground point labels (point clouds), and detailed annotations for all vehicles and people. [paper] [det. aut.] [3D Match] Keypoint Matching Benchmark, Geometric Registration Benchmark, RGB-D Reconstruction Datasets. [reg. rec. oth.] [BLVD] (a) 3D detection, (b) 4D tracking, (c) 5D interactive event recognition and (d) 5D intention prediction. [ICRA 2019 paper] [det. tra. aut. oth.] [PedX] 3D Pose Estimation of Pedestrians, more than 5,000 pairs of high-resolution (12MP) stereo images and LiDAR data along with providing 2D and 3D labels of pedestrians. [ICRA 2019 paper] [pos. aut.] [H3D] Full-surround 3D multi-object detection and tracking dataset. [ICRA 2019 paper] [det. tra. aut.] [Argoverse BY ARGO AI] Two public datasets (3D Tracking and Motion Forecasting) supported by highly detailed maps to test, experiment, and teach self-driving vehicles how to understand the world around them.[CVPR 2019 paper][tra. aut.] [Matterport3D] RGB-D: 10,800 panoramic views from 194,400 RGB-D images. Annotations: surface reconstructions, camera poses, and 2D and 3D semantic segmentations. Keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and scene classification. [3DV 2017 paper] [code] [blog] [SynthCity] SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Nine categories. [seg. aut.] [Lyft Level 5] Include high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map. [det. seg. aut.] [SemanticKITTI] Sequential Semantic Segmentation, 28 classes, for autonomous driving. All sequences of KITTI odometry labeled. [ICCV 2019 paper] [seg. oth. aut.] [NPM3D] The Paris-Lille-3D has been produced by a Mobile Laser System (MLS) in two different cities in France (Paris and Lille). [seg.] [The Waymo Open Dataset] The Waymo Open Dataset is comprised of high resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions. [det.] [A*3D: An Autonomous Driving Dataset in Challeging Environments] A*3D: An Autonomous Driving Dataset in Challeging Environments. [det.] [PointDA-10 Dataset] Domain Adaptation for point clouds. [Oxford Robotcar] The dataset captures many different combinations of weather, traffic and pedestrians. [cls. det. rec.]","link":"","categories":[{"name":"CV","slug":"CV","permalink":"https://Yansz.github.io/categories/CV/"}],"tags":[{"name":"3DSeg","slug":"3DSeg","permalink":"https://Yansz.github.io/tags/3DSeg/"}]},{"title":"PointNet++ Learning","date":"2019-12-22T21:00:00.000Z","path":"2019/12/23/PointNet++/","text":"Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, they introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds. 1. Architecture This paper introduce a type of novel neural network, named as PointNet++, to process a set of points sampled in a metric space in a hierarchical fashion (2D points in Euclidean space are used for this illustration). The general idea of PointNet++ is simple. They first partition the set of points into overlapping local regions by the distance metric of the underlying space. Similar to CNNs, they extract local features capturing fine geometric structures from small neighborhoods; such local features are further grouped into larger units and processed to produce higher level features. This process is repeated until they obtain the features of the whole point set. This structure is mainly divided into three parts: Sampling layer: used to select a part of points from the input point cloud, these points are also the centroids of the local area; Grouping Layer: Used to select points that are “adjacent” to the centroid according to the rules of the neighborhood; PointNet layer: Mini-PointNet is used to encode the graphics of local areas into feature vectors. Input: dimensions N × (d + C) , which means N points with d dimension coordinate information and C dimension features.Output: a matrix with dimension N′ × (d + C′) , which means a new C ′ with d-dimensional coordinate information and summary local information N ′ Points. 2.Non-uniform sampling density 2.1. Multi-scale grouping(MSG)As shown on the left of the figure above, at each grouping layer, each group is determined by multiple scales (set multiple radius values), and multiple features are concated after pointnet extraction of features to obtain new features. 2.2. Multi-resolution grouping(MRG)As shown on the right of the figure above. The feature vector on the left is obtained after two set abstractions, and the radius of each set abstraction is different. The feature vector on the right is obtained by directly performing pointnet convolution on all points in the current layer. In addition, when the density of the point cloud is uneven, the left and right feature vectors can be given different weights by judging the density of the current patch. For example, when the density in the patch is small, the information obtained by the left vector is not as reliable as the features extracted from all the midpoints of the patch, so the weight of the right feature vector is increased. This reduces the amount of calculations while solving the density problem. 3. classification and segmentationDivided into Set Abstraction layers, Feature Propagation layers, FC layers. A UNet-like structure, the code of the entire segmented network is as follows: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 # Set Abstraction layers l1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points, npoint=512, radius=0.2, nsample=64, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1') l2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points, npoint=128, radius=0.4, nsample=64, mlp=[128,128,256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2') l3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points, npoint=None, radius=None, nsample=None, mlp=[256,512,1024], mlp2=None, group_all=True, is_training=is_training, bn_decay=bn_decay, scope='layer3') # Feature Propagation layers l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points, [256,256], is_training, bn_decay, scope='fa_layer1') l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points, [256,128], is_training, bn_decay, scope='fa_layer2') l0_points = pointnet_fp_module(l0_xyz, l1_xyz, tf.concat([l0_xyz,l0_points],axis=-1), l1_points, [128,128,128], is_training, bn_decay, scope='fa_layer3') # FC layers net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay) end_points['feats'] = net net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1') net = tf_util.conv1d(net, 50, 1, padding='VALID', activation_fn=None, scope='fc2') def pointnet_fp_module(xyz1, xyz2, points1, points2, mlp, is_training, bn_decay, scope, bn=True): ''' PointNet Feature Propogation (FP) Module Input: xyz1: (batch_size, ndataset1, 3) TF tensor xyz2: (batch_size, ndataset2, 3) TF tensor, sparser than xyz1 points1: (batch_size, ndataset1, nchannel1) TF tensor points2: (batch_size, ndataset2, nchannel2) TF tensor mlp: list of int32 -- output size for MLP on each point Return: new_points: (batch_size, ndataset1, mlp[-1]) TF tensor ''' with tf.variable_scope(scope) as sc: dist, idx = three_nn(xyz1, xyz2) dist = tf.maximum(dist, 1e-10) norm = tf.reduce_sum((1.0/dist),axis=2,keep_dims=True) norm = tf.tile(norm,[1,1,3]) weight = (1.0/dist) / norm #weight is the inverse of distance # interpolate interpolated_points = three_interpolate(points2, idx, weight) if points1 is not None: new_points1 = tf.concat(axis=2, values=[interpolated_points, points1]) # B,ndataset1,nchannel1+nchannel2 else: new_points1 = interpolated_points new_points1 = tf.expand_dims(new_points1, 2) for i, num_out_channel in enumerate(mlp): new_points1 = tf_util.conv2d(new_points1, num_out_channel, [1,1], padding='VALID', stride=[1,1], bn=bn, is_training=is_training, scope='conv_%d'%(i), bn_decay=bn_decay) new_points1 = tf.squeeze(new_points1, [2]) # B,ndataset1,mlp[-1] return new_points1 SA module Feature extraction module: downsampling. The input is (N, D) points for N D-dimensional features, and the output is (N’, D’) for N’ points after downsampling, and each point uses the furthest point to find N’ center points. The characteristics of the N′ dimension are obtained by pointnet calculation. The previous classification network said that I won’t go into details. FP module feature transfer module: used for upsampling. Using the inverse of the distance as the weight. This interpolation of input (N, D) and output (N’, D) guarantees that the feature dimensions of the input remain unchanged.","link":"","categories":[{"name":"CV","slug":"CV","permalink":"https://Yansz.github.io/categories/CV/"}],"tags":[{"name":"3DSeg","slug":"3DSeg","permalink":"https://Yansz.github.io/tags/3DSeg/"}]},{"title":"Understanding of PointNet network architecture","date":"2019-12-19T21:00:00.000Z","path":"2019/12/20/PointNet/","text":"1. Architecture 2.Feature Transformation Networks T-NetPointNet solves two key problems: the invariance of point cloud transformation the disorder of point cloud. For the invariance of point cloud transformation, the class of the point cloud object will not change after rotation, PointNet refers to the STN in 2D deep learning on this issue, and adds T-Net Network architecture here to spatially transform the input point cloud, making it as invariant to rotation as possible. feat_transform.py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124# -*- coding: utf-8 -*-# Implementation of the Feature Transformation Networks T-Netimport osimport sysimport numpy as npimport torchimport functoolsimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variablefrom torchsummary import summary'''[description]input_transform_netInput (XYZ) Transform Net, input is Bx3xNB-&gt;Batch sizeN-&gt;number of points per batch Return: Transformation matrix of size 3x3'''class input_transform_net(nn.Module): def __init__(self): super(input_transform_net, self).__init__() self.conv1 = torch.nn.Conv1d(3, 64, 1) self.bn1 = nn.BatchNorm1d(64) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.bn2 = nn.BatchNorm1d(128) self.conv3 = torch.nn.Conv1d(128, 1024, 1) self.bn3 = nn.BatchNorm1d(1024) self.fc1 = nn.Linear(1024, 512) self.fc2 = nn.Linear(512, 256) self.fc3 = nn.Linear(256, 9) self.relu = nn.ReLU() self.bn4 = nn.BatchNorm1d(512) self.bn5 = nn.BatchNorm1d(256) def forward(self, x): batchsize = x.size()[0] x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) # The number of channels has been adjusted to 1024 # Use a maxpooling for every sample point set x = torch.max(x, 2, keepdim=True)[0] x = x.view(-1, 1024) x = F.relu(self.bn4(self.fc1(x))) x = F.relu(self.bn5(self.fc2(x))) x = self.fc3(x) # Use an Identity matrix to do initialization iden = Variable(torch.from_numpy (np.array([1, 0, 0, 0, 1, 0, 0, 0,1]) .astype(np.float32))).view(1, 9) .repeat(batchsize, 1) if x.is_cuda: #Use GPU to accelerate calculations iden = iden.cuda() x = x + iden x = x.view(-1, 3, 3) return x'''[description]feature_transform_netFeature Transform Net, input is BxKxN Return: Transformation matrix of size KxK'''class feature_transform_net(nn.Module): def __init__(self, K=64): super(feature_transform_net, self).__init__() self.K = K self.conv1 = torch.nn.Conv1d(64, 64, 1) self.bn1 = nn.BatchNorm1d(64) self.conv2 = torch.nn.Conv1d(64, 128, 1) self.bn2 = nn.BatchNorm1d(128) self.conv3 = torch.nn.Conv1d(128, 1024, 1) self.bn3 = nn.BatchNorm1d(1024) self.fc1 = nn.Linear(1024, 512) self.bn4 = nn.BatchNorm1d(512) self.fc2 = nn.Linear(512, 256) self.bn5 = nn.BatchNorm1d(256) self.fc3 = nn.Linear(256, self.K**2) self.relu = nn.ReLU() def forward(self, x): batchsize = x.size()[0] x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.relu(self.bn3(self.conv3(x))) # The number of channel has been modified to be 1024 # # maxpooling for every sample point set x = torch.max(x, 2, keepdim=True)[0] x = x.view(-1, 1024) x = F.relu(self.bn4(self.fc1(x))) x = F.relu(self.bn5(self.fc2(x))) x = self.fc3(x) iden = Variable(torch.from_numpy(np.eye(self.K) .flatten().astype(np.float32))) .view(1, self.K**2) .repeat(batchsize, 1) if x.is_cuda: iden = iden.cuda()#Use GPU to accelerate calculations x = x + iden x = x.view(-1, self.K, self.K) return x 3. Implementation of classification and segmentationThe important point is to ensure that the network is invariant to different input orders of the same point cloud. So the PointNet uses a symmetric function. In the implementation of pointnet network, the max-pooling strategy is adopted to solve the disorder problem of point cloud. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200# -*- coding: utf-8 -*-import osimport sysimport numpy as npimport torchimport functoolsimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variablefrom torchsummary import summaryfrom feat_transform import *'''[description]PointNetClassifyPointNetSegmentationinput is Bx3xN Return: BxNxK (K is the number of classes)'''class PointNetClassify(nn.Module): def __init__(self, class_num=2): super(PointNetClassify, self).__init__() self.class_num = class_num self.input_transform = input_transform_net() self.conv1 = torch.nn.Conv1d(3, 64, 1) self.bn1 = nn.BatchNorm1d(64) self.conv2 = torch.nn.Conv1d(64, 64, 1) self.bn2 = nn.BatchNorm1d(64) self.feature_transform = feature_transform_net(K=64) self.conv3 = torch.nn.Conv1d(64, 64, 1) self.bn3 = nn.BatchNorm1d(64) self.conv4 = torch.nn.Conv1d(64, 128, 1) self.bn4 = nn.BatchNorm1d(128) self.conv5 = torch.nn.Conv1d(128, 1024, 1) self.bn5 = nn.BatchNorm1d(1024) self.fc1 = nn.Linear(1024, 512) self.fc_bn1 = nn.BatchNorm1d(512) self.fc2 = nn.Linear(512, 256) self.fc_bn2 = nn.BatchNorm1d(256) self.dropout = nn.Dropout(0.7) self.fc3 = nn.Linear(256, self.class_num) self.classifier = torch.nn.Conv1d(128, self.class_num, 1) def forward(self, x): batchsize = x.size()[0] n_pts = x.size()[2] # number of points per sample input_transform = self.input_transform(x) x = x.transpose(2, 1) # Bx3xN -&gt; BxNx3 x = torch.bmm(x, input_transform) # bmm: batch matrix multiply; x -&gt; BxNx3 x = x.transpose(2, 1) # BxNx3 -&gt; Bx3xN # mlp(64, 64) x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) # -&gt; Bx64xN feature_transform = self.feature_transform(x) x = x.transpose(2, 1) # Bx64xN -&gt; BxNx64 x = torch.bmm(x, feature_transform) # bmm: batch matrix multiply; x -&gt; BxNx64 x = x.transpose(2, 1) # BxNx64 -&gt; Bx64xN # mlp(64, 128, 1024) x = F.relu(self.bn3(self.conv3(x))) x = F.relu(self.bn4(self.conv4(x))) x = F.relu(self.bn5(self.conv5(x))) # -&gt; Bx1024xN x = torch.max(x, 2, keepdim=True)[0] global_feat = x.view(-1, 1024) # global features, Bx1024 x = F.relu(self.fc_bn1(self.fc1(global_feat))) x = F.relu(self.fc_bn2(self.fc2(x))) x = self.dropout(x) x = self.fc3(x) x = F.log_softmax(x, dim=1) return x'''[description]PointNetSegmentationinput is Bx3xN Return: BxNxK (K is the class num)'''class PointNetSegmentation(nn.Module): def __init__(self, class_num=2, global_feat=False): super(PointNetSegmentation, self).__init__() self.class_num = class_num self.global_feat = global_feat self.input_transform = input_transform_net() self.conv1 = torch.nn.Conv1d(3, 64, 1) self.bn1 = nn.BatchNorm1d(64) self.conv2 = torch.nn.Conv1d(64, 64, 1) self.bn2 = nn.BatchNorm1d(64) self.feature_transform = feature_transform_net(K=64) self.conv3 = torch.nn.Conv1d(64, 64, 1) self.bn3 = nn.BatchNorm1d(64) self.conv4 = torch.nn.Conv1d(64, 128, 1) self.bn4 = nn.BatchNorm1d(128) self.conv5 = torch.nn.Conv1d(128, 1024, 1) self.bn5 = nn.BatchNorm1d(1024) # segmentation network if not self.global_feat: self.conv6 = torch.nn.Conv1d(1088, 512, 1) self.bn6 = nn.BatchNorm1d(512) else: self.conv6 = torch.nn.Conv1d(1024, 512, 1) self.bn6 = nn.BatchNorm1d(512) self.conv7 = torch.nn.Conv1d(512, 256, 1) self.bn7 = nn.BatchNorm1d(256) self.conv8 = torch.nn.Conv1d(256, 128, 1) self.bn8 = nn.BatchNorm1d(128) self.classifier = torch.nn.Conv1d(128, self.class_num, 1) def forward(self, x): batchsize = x.size()[0] n_pts = x.size()[2] # number of points per sample input_transform = self.input_transform(x) x = x.transpose(2, 1) # Bx3xN -&gt; BxNx3 x = torch.bmm(x, input_transform) # bmm: batch matrix multiply; x -&gt; BxNx3 x = x.transpose(2, 1) # BxNx3 -&gt; Bx3xN # mlp(64, 64) x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) # -&gt; Bx64xN feature_transform = self.feature_transform(x) x = x.transpose(2, 1) # Bx64xN -&gt; BxNx64 x = torch.bmm(x, feature_transform) # bmm: batch matrix multiply; x -&gt; BxNx64 x = x.transpose(2, 1) # BxNx64 -&gt; Bx64xN # to save memory del feature_transform del input_transform # Feature for per point, local feature pointfeat = x # mlp(64, 128, 1024) x = F.relu(self.bn3(self.conv3(x))) x = F.relu(self.bn4(self.conv4(x))) x = F.relu(self.bn5(self.conv5(x))) # -&gt; Bx1024xN x = torch.max(x, 2, keepdim=True)[0] x = x.view(-1, 1024) # global features, Bx1024 x = x.view(-1, 1024, 1).repeat(1, 1, n_pts) # Bx1024 -&gt; Bx1024xN # Global-feature + point-feature if not self.global_feat: x = torch.cat([x, pointfeat], 1) # mlp(512, 256, 128) x = F.relu(self.bn6(self.conv6(x))) x = F.relu(self.bn7(self.conv7(x))) x = F.relu(self.bn8(self.conv8(x))) # mlp(128, m) x = self.classifier(x) x = x.transpose(2, 1).contiguous() x = F.log_softmax(x.view(-1, self.class_num), dim=-1) x = x.view(batchsize, n_pts, self.class_num) return xif __name__ == '__main__': sim_data = Variable(torch.rand(32,3,2500)) cly = PointNetClassify(class_num = 40) out = cly(sim_data) print('cly', out.size()) seg = PointNetSegmentation(class_num = 40) out = seg(sim_data) print('seg', out.size())","link":"","categories":[{"name":"CV","slug":"CV","permalink":"https://Yansz.github.io/categories/CV/"}],"tags":[{"name":"3DSeg","slug":"3DSeg","permalink":"https://Yansz.github.io/tags/3DSeg/"}]},{"title":"数据科学家技能地铁图","date":"2019-06-10T21:00:00.000Z","path":"2019/06/11/数据科学家技能地铁图/","text":"数据科学家技能地铁图","link":"","categories":[{"name":"Data_science","slug":"Data-science","permalink":"https://Yansz.github.io/categories/Data-science/"}],"tags":[{"name":"Technical route","slug":"Technical-route","permalink":"https://Yansz.github.io/tags/Technical-route/"}]},{"title":"Logistic的代价函数的求导详细推理过程","date":"2019-05-19T21:00:00.000Z","path":"2019/05/20/Logistic损失函数推导/","text":"逻辑回归代价函数的求导详细推理过程 基本的求导公式复习 1.基本求导法则 记$$u(x)=u, v(x)=v$$$u(x)=u, v(x)=v$皆为可导函数，$$(u \\pm v)^{\\prime}=u^{\\prime} \\pm v^{\\prime}$$(1) $(u \\pm v)^{\\prime}=u^{\\prime} \\pm v^{\\prime}$(2) $(u v)^{\\prime}=u^{\\prime} v+u v^{\\prime}$(3) $(C u)^{\\prime}=C u^{\\prime}$(4) $\\left(\\frac{u}{v}\\right)^{\\prime}=\\frac{u^{\\prime} v-u v^{\\prime}}{v^{2}}$ 2.对数求导： $$\\mathrm{y}=\\log (u), y^{\\prime}=(\\log (u))^{\\prime}=\\frac{1}{u} u^{\\prime}$$ 3.幂函数求导： $$f=\\left(y^{x}\\right), f^{\\prime}=\\left(y^{x}\\right)^{\\prime}=x y^{x-1} x^{\\prime}$$4.复合函数求导数：设 $y=f(u)$，$u=\\varphi(x)$且$f(u)$及$\\varphi(x)$都可导，则复合函数$y=f[\\varphi(x)]$的导数为：$\\frac{d y}{d x}=\\frac{d y}{d u} * \\frac{d u}{d x}$$\\mathrm{y}^{\\prime}=f^{\\prime}(u) * \\varphi^{\\prime}(x)$ Logistic 回归的 Cost function 的推导过程： 之前使用的函数计算损失函数一般都是使用梯度下降法，现在我们学习新的一种方法","link":"","categories":[{"name":"ML","slug":"ML","permalink":"https://Yansz.github.io/categories/ML/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://Yansz.github.io/tags/Algorithm/"}]},{"title":"3D Point Cloud Visual","date":"2019-05-10T21:00:00.000Z","path":"2019/05/11/3D PointVisual/","text":"Point cloud viewerDependencies 123pip install numpy --userpip install mayavi --userpip install PyQt5 --user&lt;!---more---&gt; 123456789101112131415161718192021222324252627282930import numpy as npimport mayavi.mlab #Python for 3D plotting\"\"\"3D point cloud visualization\"\"\"def viz_mayavi(points, vals=\"distance\"): x = points[:, 0] # x y = points[:, 1] # y z = points[:, 2] # z # r = lidar[:, 3] # Confidence value d = np.sqrt(x ** 2 + y ** 2) # Distance from image to touch sensor if vals == \"height\": col = z else: col = d fig = mayavi.mlab.figure(bgcolor=(0, 0, 0), size=(640, 360)) mayavi.mlab.points3d(x, y, z, col, # Color value mode=\"point\", colormap='spectral', # 'bone', 'copper', 'gnuplot' # color=(0, 1, 0), # Use fixed colors figure=fig, ) mayavi.mlab.show()points = np.loadtxt('data/6.xyz')# Select dataviz_mayavi(points) Demo screenshot","link":"","categories":[{"name":"Point cloud","slug":"Point-cloud","permalink":"https://Yansz.github.io/categories/Point-cloud/"}],"tags":[{"name":"visual","slug":"visual","permalink":"https://Yansz.github.io/tags/visual/"}]},{"title":"机器学习之SVM(一)","date":"2019-03-17T21:00:00.000Z","path":"2019/03/18/机器学习之支持向量机SVM/","text":"零基础学SVM—Support Vector Machine(一)如果你是一名模式识别专业的研究生，又或者你是机器学习爱好者，SVM是一个你避不开的问题。如果你只是有一堆数据需要SVM帮你处理一下，那么无论是Matlab的SVM工具箱，LIBSVM还是python框架下的SciKit Learn都可以提供方便快捷的解决方案。但如果你要追求的不仅仅是会用，还希望挑战一下“理解”这个层次，那么你就需要面对一大堆你可能从来没听过的名词，比如：非线性约束条件下的最优化、KKT条件、拉格朗日对偶、最大间隔、最优下界、核函数等等。这些名词往往会跟随一大堆天书一般的公式。如果你稍微有一点数学基础，那么单个公式你可能看得明白，但是怎么从一个公式跳到另一个公式就让人十分费解了，而最让人糊涂的其实并不是公式推导，而是如果把这些公式和你脑子里空间构想联系起来。 我翻阅了很多关于SVM的书籍和资料，但没有找到一份材料能够在公式推导、理论介绍，系统分析、变量说明、代数和几何意义的解释等方面完整地对SVM加以分析和说明的。换言之，对于普通的一年级非数学专业的研究生而言，要想看懂SVM需要搜集很多资料，然后对照阅读和深入思考，才可能比较透彻地理解SVM算法。由于我本人也在东北大学教授面向一年级硕士研究生的《模式识别技术与应用》课程，因此希望能总结出一份相对完整、简单和透彻的关于SVM算法的介绍文字，以便学生能够快速准确地理解SVM算法。 以下我会分为四个步骤对最基础的线性SVM问题加以介绍，分别是1）问题原型，2）数学模型，3）最优化求解，4）几何解释。我尽可能用最简单的语言和最基本的数学知识对上述问题进行介绍，希望能对困惑于SVM算法的学生有所帮助。 由于个人时间有限，只能找空闲的时间更新，速度会比较慢，请大家谅解。 一、SVM算法要解决什么问题SVM的全称是Support Vector Machine，即支持向量机，主要用于解决模式识别领域中的数据分类问题，属于有监督学习算法的一种。SVM要解决的问题可以用一个经典的二分类问题加以描述。如图1所示，红色和蓝色的二维数据点显然是可以被一条直线分开的，在模式识别领域称为线性可分问题。然而将两类数据点分开的直线显然不止一条。图1(b)和(c)分别给出了A、B两种不同的分类方案，其中黑色实线为分界线，术语称为“决策面”。每个决策面对应了一个线性分类器。虽然在目前的数据上看，这两个分类器的分类结果是一样的，但如果考虑潜在的其他数据，则两者的分类性能是有差别的。 图1 二分类问题描述 SVM算法认为图1中的分类器A在性能上优于分类器B，其依据是A的分类间隔比B要大。这里涉及到第一个SVM独有的概念“分类间隔”。在保证决策面方向不变且不会出现错分样本的情况下移动决策面，会在原来的决策面两侧找到两个极限位置（越过该位置就会产生错分现象），如虚线所示。虚线的位置由决策面的方向和距离原决策面最近的几个样本的位置决定。而这两条平行虚线正中间的分界线就是在保持当前决策面方向不变的前提下的最优决策面。两条虚线之间的垂直距离就是这个最优决策面对应的分类间隔。显然每一个可能把数据集正确分开的方向都有一个最优决策面（有些方向无论如何移动决策面的位置也不可能将两类样本完全分开），而不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为“支持向量”。对于图1中的数据，A决策面就是SVM寻找的最优解，而相应的三个位于虚线上的样本点在坐标系中对应的向量就叫做支持向量。 从表面上看，我们优化的对象似乎是这个决策面的方向和位置。但实际上最优决策面的方向和位置完全取决于选择哪些样本作为支持向量。而在经过漫长的公式推导后，你最终会发现，其实与线性决策面的方向和位置直接相关的参数都会被约减掉，最终结果只取决于样本点的选择结果。 到这里，我们明确了SVM算法要解决的是一个最优分类器的设计问题。既然叫作最优分类器，其本质必然是个最优化问题。所以，接下来我们要讨论的就是如何把SVM变成用数学语言描述的最优化问题模型，这就是我们在第二部分要讲的“线性SVM算法的数学建模”。 关于“决策面”，为什么叫决策面，而不是决策线？好吧，在图1里，样本是二维空间中的点，也就是数据的维度是2，因此1维的直线可以分开它们。但是在更加一般的情况下，样本点的维度是n，则将它们分开的决策面的维度就是n-1维的超平面（可以想象一下3维空间中的点集被平面分开*），所以叫“决策面”更加具有普适性，或者你可以认为直线是决策面的一个特例。 二、线性SVM算法的数学建模一个最优化问题通常有两个最基本的因素：1）目标函数，也就是你希望什么东西的什么指标达到最好；2）优化对象，你期望通过改变哪些因素来使你的目标函数达到最优。在线性SVM算法中，目标函数显然就是那个“分类间隔”，而优化对象则是决策面。所以要对SVM问题进行数学建模，首先要对上述两个对象（“分类间隔”和“决策面”）进行数学描述。按照一般的思维习惯，我们先描述决策面。 2.1 决策面方程 （请注意，以下的描述对于线性代数及格的同学可能显得比较啰嗦，但请你们照顾一下用高数课治疗失眠的同学们。） 请你暂时不要纠结于n维空间中的n-1维超平面这种超出正常人想象力的情景。我们就老老实实地看看二维空间中的一根直线，我们从初中就开始学习的直线方程形式很简单。 (2.1) 现在我们做个小小的改变，让原来的)轴变成)轴，)变成)轴，于是公式(2.1)中的直线方程会变成下面的样子 (2.2) (2.3) 公式（2.3）的向量形式可以写成 (2.4) 考虑到我们在等式两边乘上任何实数都不会改变等式的成立，所以我们可以写出一个更加一般的向量表达形式: (2.5) 看到变量)略显粗壮的身体了吗？他们是黑体，表示变量是个向量，)，。一般我们提到向量的时候，都默认他们是个列向量，所以我在方括号[ ]后面加上了上标T，表示转置（我知道我真的很啰嗦，但是关于“零基础”三个字，我是认真的。），它可以帮忙把行向量竖过来变成列向量，所以在公式(2.5)里面)后面的转置符号T，会把列向量又转回到行向量。这样一个行向量)和一个列向量)就可快快乐乐的按照矩阵乘法的方式结合，变成一个标量，然后好跟后面的标量相加后相互抵消变成0。 就着公式(2.5)，我们再稍稍尝试深入一点。那就是探寻一下向量)和标量)的几何意义是什么。让我们回到公式(2.4)，对比公式(2.5)，可以发现此时的)。然后再去看公式(2.2)，还记得那条我们熟悉的直线方程中的a的几何意义吗？对的，那是直线的斜率。如果我们构造一个向量)，它应该跟我们的公式(2.2)描述的直线平行。然后我们求一下两个向量的点积)，你会惊喜地发现结果是0。我们管这种现象叫作“两个向量相互正交”。通俗点说就是两个向量相互垂直。当然，你也可以在草稿纸上自己画出这两个向量，比如让),你会发现)在第一象限，与横轴夹角为60°，而在第四象限与横轴夹角为30°，所以很显然他们两者的夹角为90°。 你现在是不是已经忘了我们讨论正交或者垂直的目的是什么了？那么请把你的思维从坐标系上抽出来，回到决策面方程上来。我是想告诉你向量)跟直线 是相互垂直的，也就是说)控制了直线的方向。另外，还记得小时候我们学过的那个叫做截距的名词吗？对了，就是截距，它控制了直线的位置。 然后，在本小节的末尾，我冒昧地提示一下，在n维空间中n-1维的超平面的方程形式也是公式(2.5)的样子，只不过向量的维度从原来的2维变成了n维。如果你还是想不出来超平面的样子，也很正常。那么就请你始终记住平面上它们的样子也足够了。 到这里，我们花了很多篇幅描述一个很简单的超平面方程（其实只是个直线方程），这里真正有价值的是这个控制方向的参数。接下来，你会有很长一段时间要思考它到底是个什么东西，对于SVM产生了怎样的影响。 2.2 分类“间隔”的计算模型 我们在第一章里介绍过分类间隔的定义及其直观的几何意义。间隔的大小实际上就是支持向量对应的样本点到决策面的距离的二倍，如图2所示。 图2 分类间隔计算 所以分类间隔计算似乎相当简单，无非就是点到直线的距离公式。如果你想要回忆高中老师在黑板上推导的过程，可以随便在百度文库里搜索关键词“点到直线距离推导公式”，你会得到至少6、7种推导方法。但这里，请原谅我给出一个简单的公式如下： (2.6) 这里)是向量)的模，表示在空间中向量的长度，)就是支持向量样本点的坐标。)就是决策面方程的参数。而追求)的最大化也就是寻找的最大化。看起来我们已经找到了目标函数的数学形式。 但问题当然不会这么简单，我们还需要面对一连串令人头疼的麻烦。 2.3 约束条件 接着2.2节的结尾，我们讨论一下究竟还有哪些麻烦没有解决： 1）并不是所有的方向都存在能够实现100%正确分类的决策面，我们如何判断一条直线是否能够将所有的样本点都正确分类？ 2）即便找到了正确的决策面方向，还要注意决策面的位置应该在间隔区域的中轴线上，所以用来确定决策面位置的截距也不能自由的优化，而是受到决策面方向和样本点分布的约束。 3）即便取到了合适的方向和截距，公式(2.6)里面的不是随随便便的一个样本点，而是支持向量对应的样本点。对于一个给定的决策面，我们该如何找到对应的支持向量？ 以上三条麻烦的本质是“约束条件”，也就是说我们要优化的变量的取值范围受到了限制和约束。事实上约束条件一直是最优化问题里最让人头疼的东西。但既然我们已经论证了这些约束条件确实存在，就不得不用数学语言对他们进行描述。尽管上面看起来是3条约束，但SVM算法通过一些巧妙的小技巧，将这三条约束条件融合在了一个不等式里面。 我们首先考虑一个决策面是否能够将所有的样本都正确分类的约束。图2中的样本点分成两类（红色和蓝色），我们为每个样本点)加上一个类别标签： (2.7) 如果我们的决策面方程能够完全正确地对图2中的样本点进行分类，就会满足下面的公式 (2.8) 如果我们要求再高一点，假设决策面正好处于间隔区域的中轴线上，并且相应的支持向量对应的样本点到决策面的距离为d，那么公式(2.8)就可以进一步写成： （2.9） 符号是“对于所有满足条件的” 的缩写。我们对公式(2.9)中的两个不等式的左右两边除上d，就可得到： (2.10) 其中 把 )和)就当成一条直线的方向矢量和截距。你会发现事情没有发生任何变化，因为直线)和直线)其实是一条直线。现在，现在让我忘记原来的直线方程参数)和，我们可以把参数 )和)重新起个名字，就叫它们)和。我们可以直接说：“对于存在分类间隔的两类样本点，我们一定可以找到一些决策面，使其对于所有的样本点均满足下面的条件：” （2.11） 公式(2.11)可以认为是SVM优化问题的约束条件的基本描述。 2.4 线性SVM优化问题基本描述公式(2.11)里面)的情况什么时候会发生呢，参考一下公式(2.9)就会知道，只有当)是决策面)所对应的支持向量样本点时，等于1或-1的情况才会出现。这一点给了我们另一个简化目标函数的启发。回头看看公式(2.6)，你会发现等式右边分子部分的绝对值符号内部的表达式正好跟公式(2.11)中不等式左边的表达式完全一致，无论原来这些表达式是1或者-1，其绝对值都是1。所以对于这些支持向量样本点有： （2.12） 公式(2.12)的几何意义就是，支持向量样本点到决策面方程的距离就是)。我们原来的任务是找到一组参数)使得分类间隔)最大化，根据公式(2.12)就可以转变为)的最小化问题，也等效于)的最小化问题。我们之所以要在上加上平方和1/2的系数，是为了以后进行最优化的过程中对目标函数求导时比较方便，但这绝不影响最优化问题最后的解。 另外我们还可以尝试将公式(2.11)给出的约束条件进一步在形式上精练，把类别标签和两个不等式左边相乘，形成统一的表述： （2.13） 好了，到这里我们可以给出线性SVM最优化问题的数学描述了： （2.14） 这里m是样本点的总个数，缩写s. t. 表示“Subject to”，是“服从某某条件”的意思。公式(2.14)描述的是一个典型的不等式约束条件下的二次型函数优化问题，同时也是支持向量机的基本数学模型。（此时此刻，你也许会回头看2.3节我们提出的三个约束问题，思考它们在公式2.14的约束条件中是否已经得到了充分的体现。但我不建议你现在就这么做，因为2.14采用了一种比较含蓄的方式表示这些约束条件，所以你即便现在不理解也没关系，后面随着推导的深入，这些问题会一点点露出真容。） 接下来，我们将在第三章讨论大多数同学比较陌生的问题：如何利用最优化技术求解公式(2.14)描述的问题。哪些令人望而生畏的术语，凸二次优化、拉格朗日对偶、KKT条件、鞍点等等，大多出现在这个部分。全面理解和熟练掌握这些概念当然不容易，但如果你的目的主要是了解这些技术如何在SVM问题进行应用的，那么阅读过下面一章后，你有很大的机会可以比较直观地理解这些问题。 come from future______by Chen *一点小建议，读到这里，你可以试着在纸上随便画一些点，然后尝试用SVM的思想手动画线将两类不同的点分开。你会发现大多数情况下，你会先画一条可以成功分开两类样本点的直线，然后你会在你的脑海中想象去旋转这条线，旋转到某个角度，你就会下意识的停下来，因为如果再旋转下去，就找不到能够成功将两类点分开的直线了。这个过程就是对直线方向的优化过程。对于有些问题，你会发现SVM的最优解往往出现在不能再旋转下去的边界位置，这就是约束条件的边界，对比我们提到的等式约束条件，你会对代数公式与几何想象之间的关系得到一些相对直观的印象。 三、有约束最优化问题的数学模型（Hi，好久不见）就像我们在第二部分结尾时提到的，SVM问题是一个不等式约束条件下的优化问题。绝大多数模式识别教材在讨论这个问题时都会在附录中加上优化算法的简介，虽然有些写得未免太简略，但看总比不看强，所以这时候如果你手头有一本模式识别教材，不妨翻到后面找找看。结合附录看我下面写的内容，也许会有帮助。 我们先解释一下我们下面讲解的思路以及重点关注哪些问题： 1）有约束优化问题的几何意象：闭上眼睛你看到什么？ 2）拉格朗日乘子法：约束条件怎么跑到目标函数里面去了？ 3）KKT条件：约束条件是不等式该怎么办？ 4）拉格朗日对偶：最小化问题怎么变成了最大化问题？ 5）实例演示：拉格朗日对偶函数到底啥样子？ 6）SVM优化算法的实现：数学讲了辣么多，到底要怎么用啊？ 3.1 有约束优化问题的几何意象 约束条件一般分为等式约束和不等式约束两种，前者表示为)(注意这里的跟第二章里面的样本x没有任何关系，只是一种通用的表示)；后者表示为)（你可能会问为什么不是,别着急，到KKT那里你就明白了）。 假设)（就是这个向量一共有d个标量组成），则)的几何意象就是d维空间中的d-1维曲面，如果函数)是线性的，则是个d-1维的超平面。那么有约束优化问题就要求在这个d-1维的曲面或者超平面上找到能使得目标函数最小的点，这个d-1维的曲面就是“可行解区域”。 对于不等式约束条件，，则可行解区域从d-1维曲面扩展成为d维空间的一个子集。我们可以从d=2的二维空间进行对比理解。等式约束对应的可行解空间就是一条线；不等式约束对应的则是这条线以及线的某一侧对应的区域，就像下面这幅图的样子（图中的目标函数等高线其实就是等值线，在同一条等值线上的点对应的目标函数值相同）。 图3 有约束优化问题的几何意象图 3.2 拉格朗日乘子法 尽管在3.1节我们已经想象出有约束优化问题的几何意象。可是如何利用代数方法找到这个被约束了的最优解呢？这就需要用到拉格朗日乘子法。 首先定义原始目标函数)，拉格朗日乘子法的基本思想是把约束条件转化为新的目标函数)的一部分(关于的意义我们一会儿再解释)，从而使有约束优化问题变成我们习惯的无约束优化问题。那么该如何去改造原来的目标函数)使得新的目标函数的最优解恰好就在可行解区域中呢？这需要我们去分析可行解区域中最优解的特点。 1）最优解的特点分析 这里比较有代表性的是等式约束条件（不等式约束条件的情况我们在KKT条件里再讲）。我们观察一下图3中的红色虚线（可行解空间）和蓝色虚线（目标函数的等值线），发现这个被约束的最优解恰好在二者相切的位置。这是个偶然吗？我可以负责任地说：“NO！它们温柔的相遇，是三生的宿命。”为了解释这个相遇，我们先介绍梯度的概念。梯度可以直观的认为是函数的变化量，可以描述为包含变化方向和变化幅度的一个向量。然后我们给出一个推论： 推论1：“在那个宿命的相遇点)（也就是等式约束条件下的优化问题的最优解），原始目标函数)的梯度向量)必然与约束条件的切线方向垂直。” 关于推论1的粗浅的论证如下： 如果梯度矢量)不垂直于在)点的切线方向，就会在的切线方向上存在不等于0的分量，也就是说在相遇点)附近，)还在沿着)变化。这意味在)上)这一点的附近一定有一个点的函数值比更小，那么)就不会是那个约束条件下的最优解了。*所以，梯度向量)必然与约束条件的切线方向垂直。*** 推论2：“函数的梯度方向也必然与函数自身等值线切线方向垂直。” 推论2的粗浅论证：与推论1 的论证基本相同，如果)的梯度方向不垂直于该点等值线的切线方向，就会在等值线上有变化，这条线也就不能称之为等值线了。 根据推论1和推论2，函数)的梯度方向在)点同时垂直于约束条件)和自身的等值线的切线方向，也就是说函数)的等值线与约束条件曲线)在点具有相同（或相反）的法线方向，所以它们在该点也必然相切。 让我们再进一步，约束条件)也可以被视为函数)的一条等值线。按照推论2中“函数的梯度方向必然与自身的等值线切线方向垂直”的说法，函数)在)点的梯度矢量)也与的切线方向垂直。 到此我们可以将目标函数和约束条件视为两个具有平等地位的函数，并得到推论3： 推论3：“函数)与函数)的等值线在最优解点处相切，即两者在点的梯度方向相同或相反”， 于是我们可以写出公式(3.1)，用来描述最优解的一个特性： (3.1) 这里增加了一个新变量),用来描述两个梯度矢量的长度比例。那么是不是有了公式（3.1）就能确定)的具体数值了呢？显然不行！从代数解方程的角度看，公式（3.1）相当于d个方程（假设)是d维向量，函数)的梯度就是d个偏导数组成的向量，所以公式(2.15)实际上是1个d维矢量方程，等价于d个标量方程），而未知数除了)的d个分量以外，还有1个)。所以相当于用d个方程求解d+1个未知量，应有无穷多组解；从几何角度看，在任意曲线)（k为值域范围内的任意实数）上都能至少找到一个满足公式(3.1)的点，也就是可以找到无穷多个这样的相切点。所以我们还需要增加一点限制，使得无穷多个解变成一个解。好在这个限制是现成的，那就是： (3.2) 把公式(3.1)和(3.2)放在一起，我们有d+1个方程，解d+1个未知数，方程有唯一解，这样就能找到这个最优点了。 2）构造拉格朗日函数 虽然根据公式(3.1)和(3.2),已经可以求出等式约束条件下的最优解了，但为了在数学上更加便捷和优雅一点，我们按照本节初提到的思想，构造一个拉格朗日函数，将有约束优化问题转为无约束优化问题。拉格朗日函数具体形式如下： (3.3) 新的拉格朗日目标函数有两个自变量)，根据我们熟悉的求解无约束优化问题的思路，将公式(3.3)分别对)求导，令结果等于零，就可以建立两个方程。同学们可以自己试一下，很容易就能发现这两个由导数等于0构造出来的方程正好就是公式(3.1)和(3.2)。说明新构造的拉格朗日目标函数的优化问题完全等价于原来的等式约束条件下的优化问题。 至此，我们说明白了“为什么构造拉格朗日目标函数可以实现等式约束条件下的目标优化问题的求解”。可是，我们回头看一下公式(2.14)，也就是我们的SVM优化问题的数学表达。囧，约束条件是不等式啊！怎么办呢？ 3.3 KKT条件 对于不等式约束条件)的情况，如图4所示，最优解所在的位置)有两种可能，或者在边界曲线)上或者在可行解区域内部满足不等式的地方。 第一种情况：最优解在边界上，就相当于约束条件就是)。参考图4，注意此时目标函数)的最优解在可行解区域外面，所以函数)在最优解)附近的变化趋势是“在可行解区域内侧较大而在区域外侧较小”，与之对应的是函数)在可行解区域内小于0，在区域外大于零，所以在最优解)附近的变化趋势是内部较小而外部较大。这意味着目标函数)的梯度方向与约束条件函数)的梯度方向相反。因此根据公式(3.1)，可以推断出参数. 图4：不等式约束条件下最优解位置分布的两种情况 第二种情况：如果在区域内，则相当于约束条件没有起作用，因此公式(3.3)的拉格朗日函数中的参数)。整合这两种情况，可以写出一个约束条件的统一表达，如公式(3.4)所示。 (3.4) 其中第一个式子是约束条件本身。第二个式子是对拉格朗日乘子)的描述。第三个式子是第一种情况和第二种情况的整合：在第一种情况里，)；在第二种情况下，)。所以无论哪一种情况都有)。公式(3.4)就称为Karush-Kuhn-Tucker条件，简称KKT条件。 推导除了KKT条件，感觉有点奇怪。因为本来问题的约束条件就是一个，怎么这个KKT条件又多弄出来两条，这不是让问题变得更复杂了吗？这里我们要适当的解释一下： 1）KKT条件是对最优解的约束，而原始问题中的约束条件是对可行解的约束。 2）KKT条件的推导对于后面马上要介绍的拉格朗日对偶问题的推导很重要。 3.4 拉格朗日对偶 接下来让我们进入重头戏——拉格朗日对偶。很多教材到这里自然而然的就开始介绍“对偶问题”的概念，这实际上是一种“先知式”的教学方式，对于学生研究问题的思路开拓有害无益。所以，在介绍这个知识点之前，我们先要从宏观的视野上了解一下拉格朗日对偶问题出现的原因和背景。 按照前面等式约束条件下的优化问题的求解思路，构造拉格朗日方程的目的是将约束条件放到目标函数中，从而将有约束优化问题转换为无约束优化问题。我们仍然秉承这一思路去解决不等式约束条件下的优化问题，那么如何针对不等式约束条件下的优化问题构建拉格朗日函数呢？ 因为我们要求解的是最小化问题，所以一个直观的想法是如果我能够构造一个函数，使得该函数在可行解区域内与原目标函数完全一致，而在可行解区域外的数值非常大，甚至是无穷大，那么这个没有约束条件的新目标函数的优化问题就与原来有约束条件的原始目标函数的优化是等价的问题。 拉格朗日对偶问题其实就是沿着这一思路往下走的过程中，为了方便求解而使用的一种技巧。于是在这里出现了三个问题：1）有约束的原始目标函数优化问题；2）新构造的拉格朗日目标函数优化问题；3）拉格朗日对偶函数的优化问题。我们希望的是这三个问题具有完全相同的最优解，而在数学技巧上通常第三个问题——拉格朗日对偶优化问题——最好解决。所以拉格朗日对偶不是必须的，只是一条捷径。 1）原始目标函数（有约束条件） 为了接下来的讨论，更具有一般性，我们把等式约束条件也放进来，进而有约束的原始目标函数优化问题重新给出统一的描述： (3.5) 公式(3.5)表示m个等式约束条件和n个不等式约束条件下的目标函数的最小化问题。 2）新构造的目标函数（没有约束条件） 接下来我们构造一个基于广义拉格朗日函数的新目标函数，记为： （3.6） 其中为广义拉格朗日函数，定义为： （3.7） 这里，)，是我们在构造新目标函数时加入的系数变量，同时也是公式(3.6)中最大化问题的自变量。将公式(3.7)带入公式(3.6)有： （3.8） 我们对比公式(3.5)中的约束条件，将论域范围分为可行解区域和可行解区域外两个部分对公式（3.8）的取值进行分析，将可行解区域记为)，当时有： 可行解区域内：由于)， 且系数, 所以有： (3.9) 可行解区域外：代表公式(3.5)中至少有一组约束条件没有得到满足。如果)，则调整系数)就可以使)；如果)，调整系数)就可以使。这意味着，此时有 (3.10) 把公式(3.8),(3.9)和(3.10)结合在一起就得到我们新构造的目标函数的取值分布情况： （3.11） 此时我们回想最初构造新目标函数的初衷，就是为了建立一个在可行解区域内与原目标函数相同，在可行解区域外函数值趋近于无穷大的新函数。看看公式（3.11）,yeah,我们做到了。 现在约束条件已经没了，接下来我们就可以求解公式(3.12)的问题 （3.12） 这个问题的解就等价于有约束条件下的原始目标函数最小化问题（公式3.5）的解。 3）对偶问题 尽管公式(3.12)描述的无约束优化问题看起来很美好，但一旦你尝试着手处理这个问题，就会发现一个麻烦。什么麻烦呢？那就是我们很难建立)的显示表达式。如果再直白一点，我们很难直接从公式(3.8)里面把)这两组参数拿掉，这样我们就没法通过令)的方法求解出最优解。 要解决这个问题，就得用一点数学技巧了，这个技巧就是对偶问题。我们先把公式(3.6)和公式(3.12)放在一起，得到关于新构造的目标函数的无约束优化的一种表达： （3.13） 然后我们再构造另一个函数，叫做，然后给出另外一个优化问题的描述： (3.14) 对比公式(3.13)和(3.14)，发现两者之间存在一种对称的美感。所以我们就把(3.14)称作是(3.13)的对偶问题。现在我们可以解释一下)中的P是原始问题Primary的缩写，)中的D是对偶问题Dual的缩写。如果我们能够想办法证明(3.14)和(3.13)存在相同的解，那我们就可以在对偶问题中选择比较简单的一个来求解。 4）对偶问题同解的证明 对偶问题和原始问题到底有没有相同的最优解呢？关于这个问题的根本性证明其实没有在这里给出，而且在几乎我看到的所有有关SVM的资料里都没有给出。但我比较厚道的地方是我至少可以告诉你哪里能找到这个证明。在给出证明的链接地址之前，我们先给一个定理，帮助大家做一点准备，同时也减少一点看那些更简略的资料时的困惑。 定理一：对于任意)和有： 定理一的证明： ， 即 所以 即： 这里的分别是对偶问题和原始问题的最优值。 定理一既引入了)的概念，同时也描述了两者之间的关系。我们可以在这个基础上再给一个推论：如果能够找到一组)使得，那么就应该有： 这个推论实际上已经涉及了原始问题与对偶问题的“强对偶性”。当)时，我们称原始问题与对偶问题之间“弱对偶性”成立；若，则称“强对偶性”成立。 如果我们希望能够使用拉格朗日对偶问题替换原始问题进行求解，则需要“强对偶性”作为前提条件。于是我们的问题变成了什么情况下，强对偶性才能够在SVM问题中成立。关于这个问题我们给出定理二： 定理二：对于原始问题和对偶问题，假设函数)和不等式约束条件)为凸函数，等式约束条件中的)为仿射函数（即由一阶多项式构成的函数，)，)均为列向量，)为标量）；并且至少存在一个)使所有不等式约束条件严格成立，即)，则存在)使得)是原始问题的最优解，)是对偶问题的最优解且有：，并其充分必要条件如下： (3.15) 再次强调一下，公式(3.15)是使)为原始问题的最优解，)为对偶问题的最优解，且)的充分必要条件。公式(3.15)中的(1)(3)，是为了求解最优化要求目标函数相对于三个变量)的梯度为0；(4)(6)为KKT条件（见公式3.4(3)），这也是我们为什么要在3.3节先介绍KKT条件的原因；(7)为等式约束条件。 定理二的证明详见 《Convex Optimization》， by Boyd and Vandenberghe. Page-234, 5.3.2节。http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf 关于拉格朗日对偶的一些参考资料： \\1. 简易解说拉格朗日对偶（Lagrange duality），这一篇对对偶问题的来龙去脉说的比较清楚，但是在强对偶性证明方面只给出了定理，没有给出证明过程，同时也缺少几何解释。 2.优化问题中的对偶性理论，这一篇比较专业，关于对偶理论的概念，条件和证明都比较完整，在数学专业文献里属于容易懂的，但在我们这种科普文中属于不太好懂的，另外就是论述过程基本跟SVM没啥关系。 3.5 拉格朗日对偶函数示例 尽管上述介绍在代数层面已经比较浅显了，但是没有可视化案例仍然不容易建立起直观的印象。所以我尽可能的在这里给出一个相对简单但是有代表性的可视化案例。 图5：有约束条件下的最优化问题可视化案例。 图5中的优化问题可以写作： (3.16) 之所以说这个案例比较典型是因为它与线性SVM的数学模型非常相似，且包含了等式和不等式两种不同的约束条件。更重要的是，这两个约束条件在优化问题中都起到了作用。如图5所示，如果没有任何约束条件，最优解在坐标原点(0, 0)处（青色X）；如果只有不等式约束条件 )，最优解在坐标(1,0)处（红色X）；如果只有等式约束条件 ，最优解在坐标(1,-1)处（绿色+）；如果两个约束条件都有，最优解在 处(黄色O)。 针对这一问题，我们可以设计拉格朗日函数如下： (3.17) 根据公式（3.11），函数 只在绿色直线在红色圆圈内的部分——也就是直线 在圆 上的弦——与原目标函数 取相同的值，而在其他地方均有 ，如图6所示。 图6： （除了图中绿色虚线部分，其他部分的函数值均为无穷大）。 （需要注意的是，此处不能使用对 求导等于0的方式消掉 ，因为函数 在 为确定值时，是 的线性函数，其极大值并不在梯度为0的地方）。由于函数 在没有约束条件下的最优解并不在这条弦上，所以显然对 求导等于零的方法是找不到最优解 的。但是对于这个简单的问题，还是能够从图中看到最优解应该在 ： 由于该最优解是在 和 的交点处，所以可以很容易地理解：当 时，无论 取什么值都可以使函数 达到最小值。 然而这个最优解是依靠几何推理的方式找到的，对于复杂的问题，这种方法似乎不具有可推广性。 那么，我们不妨尝试一下，用拉格朗日对偶的方式看看这个问题。我们将 视为常数，这时 就只是 的函数。我们可以通过求导等于零的方式寻找其最小值，即 。我们对公式(3.17)对 分别求偏导，令其等于0，有： (3.18) 可以解得: (3.19) 将(3.19)带入(3.17)可以得到: (3.20) 考虑到（3.15）中的条件（5），我们将函数(3.20)在 的 论域画出来，如图7所示。可以通过 对 求导等于0的方式解出最优解 ，将其带入公式（3.19）可以得到 最后通过对比，我们看到拉格朗日原始问题和对偶问题得到了相同的最优解（原始问题的最优解中 可以是任何值）。 最后，我来解释一下鞍点的问题。鞍点的概念大家可以去网上找，形态上顾名思义，就是马鞍的中心点，在一个方向上局部极大值，在另一个方向上局部极小值。这件事跟我们的拉格朗日函数有什么关系呢？由于这个例子中的拉格朗日函数包含 四个自变量，无法直接显示。为了更好的可视化，我们固定住其中两个变量，令 。此时拉格朗日函数就变成一个可以可视化的二元函数 ，我们把它的曲面画出来。 图8： 可视化效果 图8(a)中的最优点 )可以能够两个角度去定义，如图8(b)所示。(为加以区别二维和四维的情况，我们将四维情况对应的 大写的下角标P和D改写为小写的p和d)。 第一种定义：沿着与 轴平行的方向将曲面切成无数条曲线（红色虚线），在每条红色虚线上找到最大值（绿色圆点），即 ，然后在所有的 找到最小的那个（蓝色圆点），即 。 第二种定义：沿着与 轴平行的方向将曲面切成无数条曲线（绿色虚线），在每条绿色虚线上找到最小值（红色圆点），即 ，然后在所有的 中找到最大的那个（蓝色圆点），即 。 从图8的二维情况思考神秘的四维空间中的拉格朗日函数， 就变成了 ， ，如图8(b)所示。其实四元函数 就是一个定义在4维空间上的鞍形函数，这个从两种思路共同得到的蓝色点就是函数 的鞍点，也就是我们要找的最优解。在这个二元化的图中，拉格朗日对偶问题和拉格朗日原始问题的差别就是：原始问题采用第一种定义去求解鞍点，对偶问题采用第二种方法去求解鞍点。 至此，我们比较形象地描述了一个有约束条件下的函数优化问题的拉格朗日对偶问题求解过程以及相应的几何解释。 参考文献地址：(https://www.zhihu.com/people/chen-dong-yue)","link":"","categories":[{"name":"ML","slug":"ML","permalink":"https://Yansz.github.io/categories/ML/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://Yansz.github.io/tags/Algorithm/"}]},{"title":"sk-learn,Boston房价预测","date":"2019-03-14T21:00:00.000Z","path":"2019/03/15/sk-learn,Boston房价预测/","text":"基于sk-learn_多种模型对Boston房价预测对比使用sk-learn进行房价预测的完整过程。 [TOC] 1.导入需要的模块1234567891011121314151617181920212223import numpy as npimport pandas as pdfrom pandas import Series,DataFrameimport matplotlib.pyplot as plt%matplotlib inlineimport sklearn.datasets as datasets#机器算法模型from sklearn.neighbors import KNeighborsRegressor#KNNfrom sklearn.linear_model import LinearRegression#from sklearn.linear_model import SGDRegressorfrom sklearn.linear_model import Ridgefrom sklearn.linear_model import Lassofrom sklearn.tree import DecisionTreeRegressorfrom sklearn.svm import SVR#切割训练数据和样本数据from sklearn.model_selection import train_test_split#用于模型评分from sklearn.metrics import r2_score 2.生成训练数据和测试数据123456boston = datasets.load_boston()train = boston.datatarget = boston.target#切割数据样本集合测试集X_train,x_test,y_train,y_true = train_test_split(train,target,test_size=0.2) 3.创建学习模型123456knn = KNeighborsRegressor()linear = LinearRegression()ridge = Ridge()lasso = Lasso()decision = DecisionTreeRegressor()svr = SVR() 4.训练模型123456knn.fit(X_train,y_train)linear.fit(X_train,y_train)ridge.fit(X_train,y_train)lasso.fit(X_train,y_train)decision.fit(X_train,y_train)svr.fit(X_train,y_train) 5.预测模型123456y_pre_knn = knn.predict(x_test)y_pre_linear = linear.predict(x_test)y_pre_ridge = ridge.predict(x_test)y_pre_lasso = lasso.predict(x_test)y_pre_decision = decision.predict(x_test)y_pre_svr = svr.predict(x_test) 6.评分1234567knn_score = r2_score(y_true,y_pre_knn)linear_score=r2_score(y_true,y_pre_linear)ridge_score=r2_score(y_true,y_pre_ridge)lasso_score=r2_score(y_true,y_pre_lasso)decision_score=r2_score(y_true,y_pre_decision)svr_score=r2_score(y_true,y_pre_svr)display(knn_score,linear_score,ridge_score,lasso_score,decision_score,svr_score) 7.绘图1234567891011121314151617181920212223242526272829#KNNplt.plot(y_true,label='true')plt.plot(y_pre_knn,label='knn')plt.legend()#Linearplt.plot(y_true,label='true')plt.plot(y_pre_linear,label='linear')plt.legend()#Ridgeplt.plot(y_true,label='true')plt.plot(y_pre_ridge,label='ridge')plt.legend()#lassoplt.plot(y_true,label='true')plt.plot(y_pre_lasso,label='lasso')plt.legend()#decisionplt.plot(y_true,label='true')plt.plot(y_pre_decision,label='decision')plt.legend()#SVRplt.plot(y_true,label='true')plt.plot(y_pre_svr,label='svr')plt.legend()","link":"","categories":[{"name":"ML","slug":"ML","permalink":"https://Yansz.github.io/categories/ML/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"https://Yansz.github.io/tags/sklearn/"}]},{"title":"BERT简要介绍","date":"2019-02-10T21:00:00.000Z","path":"2019/02/11/BERT简要介绍/","text":"BERT(Bidirectional Encoder Representation from Transformers) 网络架构由 Jacob Devlin 等人提出用于预训练，学习在给定上下文下给出词的 Embedding 表示。BERT 采用了 Transformer 架构的编码器部分用于学习词在给定上下文下词的 Embedding 表示。考虑到语言模型任务为从左往右或从右往左预测下一个词的自回归任务，因此若采用该模式每个词无法依赖于当前上下文下后续词进行词向量表示。为了解决该问题，BERT 提出了两类预训练任务： 掩码语言模型任务 下一句预测任务 下面我们分别来看这两类任务。 1.掩码语言模型任务若采用 self-attention 并直接用于语言模型任务，则每次预测下一个词时由于 self-attention机制会将下一个词的信息引入到当前词的表征中，当使用该词的表征用于预测下一个词时，相当于将标注引入到了特征中，因此出现学习失效问题。为了解决该问题，BERT 提出了如下解决方案：从待预测序列中随机选择 15% 的位置用于预测任务 80% 的概率下将选取出的 15% 的位置对应的词替换为 [MASK] 10% 的概率下将选取出的 15% 的位置对应的词替换为随机词 10% 的情况下不对选取出的 15% 的位置对应的词进行词替换 引入 1.5% 的随机词，相当于对数据增加部分噪音，提升模型的鲁棒性。1.5% 的情况下保留原词是因为 fine-tuning 阶段并没有 [MASK] 词。 2.编码器自注意力下一句预测任务考虑到重要的下游任务譬如问答 (Question Answering) 任务，自然语言推理 (Natural Language Inference) 任务依赖于对两个句子的关系的理解，该信息在语言模型中没有直接体现。因此 BERT 中同时设计了下一句预测任务，该任务的构建如下： 每个预训练序列由句子 A 和句子 B 构成 50% 的概率下句子 B 为句子 A 的下一个句子 50% 的概率下句子 B 不是句子 A 的下一个句子 实际构建预训练任务时是是首选设计好” 下一句预测任务”，生成该任务的标注信息，在此基础上构建” 掩码语言模型任务”，生成掩码语言模型的标注信息。考虑到预训练涉及两个句子，BERT 采用如下的输入信息表征方案： 预训练阶段损失函数通过线性加权方法，同时进行上述两类任务训练。预训练阶段结束后将学习到每个词在特定上下文中 BERT 的表征信息，该表征信息即可用于下游的任务，如下是 BERT 表征用于不同类型的下游任务的 fine-tuning 方案:","link":"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://Yansz.github.io/categories/NLP/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://Yansz.github.io/tags/BERT/"}]},{"title":"Transformer","date":"2019-02-09T21:00:00.000Z","path":"2019/02/10/Transformer详细讲解/","text":"本篇整理 Transformer 架构，及在 Transformer 基础上衍生出来的 BERT 模型，最后给出相应的应用案例。 1.Transformer的架构Transformer 网络架构架构由 Ashish Vaswani 等人在 [Attention Is All You Need]一文中提出，并用于机器翻译任务，和以往网络架构有所区别的是，该网络架构中，编码器和解码器没有采用 RNN 或 CNN 等网络架构，而是采用完全依赖于注意力机制的架构。网络架构如下所示： 该网络架构中引入了多头注意力机制，该机制的网络架构如下所示： 这里有必要对多头注意力机制进行一定的解释。假设输入数据的 batch size 为$B$，输入数据的最大长度为$F$，输出数据的最大长度为$T$，共有$N$个注意力头，每个注意力头的输出维度为$H$，则输入/输出数据中每个词的 Embedding 的维度为 $E=N×H$，且注意力头中每个头对应的$\\boldsymbol{W}^{Q}, \\boldsymbol{W}^{K} ,\\boldsymbol{W}^{V}$矩阵均属于$\\mathbb{R}^{E \\times H}$。考虑到编码器和解码器涉及三个注意过程，且输入有所不同，这里分别来看。 2.编码器自注意力考虑输入数据为$\\mathbf{X} \\in \\mathbb{R}^{B \\times F \\times E}$，对输入数据应用如下线性变换：$$\\begin{aligned}&amp;\\mathbf{Q}=\\mathbf{X} \\mathbf{W}^{Q}, \\quad\\left(\\mathbf{W}^{Q} \\in \\mathbb{R}^{E \\times H} \\Rightarrow \\mathbf{Q} \\in \\mathbb{R}^{B \\times F \\times H}\\right)\\&amp;\\mathbf{K}=\\mathbf{X} \\mathbf{W}^{K}, \\quad\\left(\\mathbf{W}^{K} \\in \\mathbb{R}^{E \\times H} \\Rightarrow \\mathbf{K} \\in \\mathbb{R}^{B \\times F \\times H}\\right)\\&amp;\\mathbf{V}=\\mathbf{X} \\boldsymbol{W}^{V}, \\quad\\left(\\boldsymbol{W}^{V} \\in \\mathbb{R}^{E \\times H} \\Rightarrow \\mathbf{V} \\in \\mathbb{R}^{B \\times F \\times H}\\right)\\end{aligned}$$在上述变换基础上进行如下计算，得到输入中每个词和自身及其他词之间的关系权重$$\\mathbf{S}=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{H}}\\right)$$上述变换 $K^T$ 表示对张量的最内部矩阵进行转置，因此$\\mathbf{K}^{\\top} \\in \\mathbb{R}^{B \\times H \\times F}$，$\\mathrm{QK}^{\\top}$表示相同维度下张量 Q 和张量 $K^T$ 最内部矩阵执行矩阵乘法运算 (即 numpy.matmul 运算)，因此有$\\mathbf{S} \\in \\mathbb{R}^{B \\times F \\times F}$，该张量表示输入数据中每个词和自身及其他词的关系权重，每一行的得分之和为 1，即$$\\forall i, j \\quad \\operatorname{np.sum}(\\mathbf{S}[i, j,:])=1$$基于该得分即可得到，每个词在当前上下文下的新的向量表示，公式如下：$$\\mathbf{x}^{h}=\\mathbf{S V} \\quad \\Rightarrow \\mathbf{X}^{h} \\in \\mathbb{R}^{B \\times F \\times H}$$考虑到 Transformer 采用了 N 个注意力头，因此最终产生了集合大小为 N 的注意力集合$\\left{\\mathbf{X}^{h_{1}}, \\ldots, \\mathbf{X}^{h_{N}}\\right}$，将该集合中中的所有张量按照最后一个维度进行拼接，并采用矩阵$\\boldsymbol{W}^{O} \\in \\mathbb{R}^{E \\times E}$进行变换，得到最终生成的自注意力输入数据，公式如下：$$\\mathbf{X}^{a}=\\text { numpy.concatenate }\\left(\\left(\\mathbf{X}^{\\mathrm{h}{1}}, \\ldots, \\mathbf{X}^{\\mathrm{h}{\\mathrm{N}}}\\right), \\text { axis }=-1\\right) \\boldsymbol{W}^{O}$$因此有$\\mathbf{X}^{a} \\in \\mathbb{R}^{B \\times F \\times E}$。 考虑到多头注意力可以并行运算，为了充分发挥向量化计算并行效率，实际实现中往往采用如下表示方案：$$\\begin{aligned}&amp;\\mathbf{X}^{\\text {par }}=\\text { reshape }(\\mathbf{X}, \\text { to shape }=[B \\times F, N \\times H])\\&amp;\\begin{array}{ll}{\\mathbf{Q}^{p a r}=\\mathbf{X}^{p a r} \\boldsymbol{W}^{Q^{p a r}}} &amp; {\\left(\\boldsymbol{W}^{Q^{p a r}} \\in \\mathbb{R}^{(N \\times H) \\times(N \\times H)} \\Rightarrow \\mathbf{Q}^{p a r} \\in \\mathbb{R}^{(B \\times F) \\times(N \\times H)}\\right)} \\{\\mathbf{K}^{p a r}=\\mathbf{X}^{p a r} \\boldsymbol{W}^{K^{p a r}}} &amp; {\\left(\\boldsymbol{W}^{K^{p a r}} \\in \\mathbb{R}^{(N \\times H) \\times(N \\times H)} \\Rightarrow \\mathbf{K}^{p a r} \\in \\mathbb{R}^{(B \\times F) \\times(N \\times H)}\\right)} \\{\\text { V }^{p a r}=\\mathbf{X}^{p a r} \\boldsymbol{W}^{V^{p a r}}} &amp; {\\left(\\boldsymbol{W}^{V^{p a r}} \\in \\mathbb{R}^{(N \\times H) \\times(N \\times H)} \\Rightarrow \\mathbf{V}^{p a r} \\in \\mathbb{R}^{(B \\times F) \\times(N \\times H)}\\right)}\\end{array}\\end{aligned}$$在上述并行计算基础上通过如下计算得到词和自身及其他词的关系权值：$$\\begin{array}{l}{\\left.\\mathbf{v}^{p a r}=\\text { numpy.reshape (Vpar, }(B, F, N, H)\\right)} \\{ \\text { v’art }\\left.=\\text { numpy. transpose (V }^{p a r}, \\text { axes }=[0,2,1,3]\\right) \\Rightarrow \\mathbf{V}^{p a r^{t}} \\in \\mathbb{R}^{B \\times N \\times F \\times H}} \\{\\mathbf{X}^{h}=\\mathbf{S}^{p a r} \\mathbf{v}^{p a r^{t}} \\Rightarrow \\mathbf{X}^{h} \\in \\mathbb{R}^{B \\times N \\times F \\times H}} \\{\\mathbf{X}^{h}=\\text { numpy. transpose }\\left(\\mathbf{X}^{h}, \\text { axes }=[0,2,1,3]\\right) \\Rightarrow \\mathbf{X}^{h} \\in \\mathbb{R}^{B \\times F \\times N \\times H}} \\{\\left.\\mathbf{X}^{h}=\\text { numpy.reshape }\\left(\\mathbf{X}^{h}, F, N \\times H\\right)\\right) \\Rightarrow \\mathbf{X}^{h} \\in \\mathbb{R}^{B \\times F \\times E}}\\end{array}$$ 3.解码器自注意力解码器的自注意力和编码器的自注意力基本完全一致，需要注意的是解码过程是one by one的生成过程，因此输出数据中的每个词在进行自注意力的过程时，仅可以看到当前输出位置的所有前驱词的信息，因此需要对输出数据中的词进行掩码操作，该操作即对应上面的左图上的掩码操作。该掩码操作相当于执行如下操作：$$\\begin{aligned}\\mathbf{A} &amp;=\\mathbf{Q K}^{\\top}+\\mathbf{M} \\\\mathbf{S}^{p a r} &amp;=\\operatorname{softmax}\\left(\\frac{\\mathbf{A}}{\\sqrt{H}}\\right) \\Rightarrow \\mathbf{S}^{p a r} \\in \\mathbb{R}^{B \\times N \\times T \\times T}\\end{aligned}$$其中$\\mathbf{M} \\in \\mathbb{R}^{1 \\times 1 \\times T \\times T}$ 为掩码，其最内部矩阵为方阵，该方阵主对角线及以下元素均为 0，主对角线以上元素为$-\\infty$。譬如 T = 5 时，最内部方阵内容如下：$$\\boldsymbol{M}=\\left[\\begin{array}{ccccc}{0} &amp; {-\\infty} &amp; {-\\infty} &amp; {-\\infty} &amp; {-\\infty} \\{0} &amp; {0} &amp; {-\\infty} &amp; {-\\infty} &amp; {-\\infty} \\{0} &amp; {0} &amp; {0} &amp; {-\\infty} &amp; {-\\infty} \\{0} &amp; {0} &amp; {0} &amp; {0} &amp; {-\\infty} \\{0} &amp; {0} &amp; {0} &amp; {0} &amp; {0}\\end{array}\\right]$$其余操作和编码器自注意力机制一致，唯一不同的是此时需要向上面那样将输入数据换成 $\\mathbf{Y} \\in \\mathbb{R}^{N \\times T \\times E}$，因此所有的 $F$ 均需换成 $T$。 4.编码解码注意力编码解码注意力和自注意力类似，唯一不同的是计算 Q, K, V 使用的数据有所区别，计算Q 时采用 Y，计算 K 和 V 时采用 X，因此有：$$\\begin{aligned}\\mathbf{X}^{\\text {par }} &amp;=\\text {reshape}(\\mathbf{X}, \\text { to_shape }=[B \\times F, N \\times H]) \\\\mathbf{Y}^{\\text {par }} &amp;=\\text {reshape}(\\mathbf{Y}, \\text { to shape }=[B \\times T, N \\times H]) \\\\mathbf{Q}^{\\text {ende-par}} &amp;=\\mathbf{Y}^{\\text {par}} \\boldsymbol{W}^{Q \\text {ende-par}} \\quad\\left(\\boldsymbol{W}^{Q \\text { ende-par }} \\in \\mathbb{R}^{(N \\times H) \\times(N \\times H)}\\right) \\\\mathbf{K}^{\\text {ende-par}} &amp;=\\mathbf{X}^{\\text {par}} \\boldsymbol{W}^{K^{\\text {ende-par}}} \\quad\\left(\\boldsymbol{W}^{K \\text {ende-par}} \\in \\mathbb{R}^{(N \\times H) \\times(N \\times H)}\\right) \\\\mathbf{V}^{\\text {ende-par}} &amp;\\mathbf{X}^{\\text {par }} \\boldsymbol{W}^{\\text {Vpar}} \\quad\\left(\\boldsymbol{W}^{\\text {Vende-par}} \\in \\mathbb{R}^{(N \\times H) \\times(N \\times H)}\\right)\\end{aligned}$$因此有：$$\\mathbf{S}^{\\text {ende-par}}=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q}^{\\text {ende-par}^{t}} \\mathbf{K}^{\\text {ende-par}^{t \\top}}}{\\sqrt{H}}\\right) \\Rightarrow \\mathbf{S}^{\\text {ende-par}} \\in \\mathbb{R}^{B \\times N \\times T \\times F}$$ $$\\mathbf{y}^{\\text {ende-h}}=\\mathbf{S}^{\\text {ende-par}} \\mathbf{V}^{\\text {ende-par}} \\Rightarrow \\mathbf{Y}^{\\text {ende-h}} \\in \\mathbb{R}^{B \\times N \\times T \\times H}$$ 其余计算过程和编码器自注意力机制类似。","link":"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://Yansz.github.io/categories/NLP/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://Yansz.github.io/tags/Transformer/"}]},{"title":"GitHub历史版本回退","date":"2019-01-19T21:00:00.000Z","path":"2019/01/20/GitHub历史版本回退/","text":"问题描述 GitHub提交常常遇到些问题，导致之前提交的更新丢失，我们可以考虑回退到之前的版本。在GitHub的Web页面上并没有找到回退的解决方案。 本地Git Bash操作 git log main.js 首先要找到要回滚的版本的hash值 git checkout 2d1ed0 main.js 利用 hash 回滚特定文件 git commit -m &#39;回滚main.js&#39; 回滚后需要提交 git push origin master提交到远程仓库 IDEA操作 查找 commit id： 在github中每个提交的版本都对应一个独有的hash版本号，浏览GitHub上的提交历史记录，从commit中找到要回退的版本，复制commit id。 恢复历史版本： 1git reset --hard [你的commit id]1 push：推送到GitHub远程仓库 1git push -f -u origin master","link":"","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"https://Yansz.github.io/tags/Git/"}]},{"title":"CRF","date":"2019-01-15T09:02:55.000Z","path":"2019/01/15/序列标注三大模型HMM、MEMM、CRF/","text":"NLP-序列标注三大模型HMM、MEMM、CRF对于HMM，MEMM，CRF需要结合概率图的层面上理解和记忆，这几个模型主要用在了序列标注上，这篇笔记针对这三个模型进行公式推导，加深理解。 生成模型 &amp; 判别模型生成模型生成模型学习的是联合概率$p(x,y)=p(x|y)p(y)$，然后利用贝叶斯定理来求解后验概率$p(y|x)$ $$p(y|x) = \\frac{p(x|y)p(y)}{\\sum_{y{}’ \\epsilon \\gamma} p(x|y{}’)p(y{}’)}$$ 常见的生成模型有：朴素贝叶斯（NB），HMM 判别式模型直接学习$p(y|x)$，判别式模型根据所提供的feature直接学习一个分类边界 常见的判别式模型：神经网络，SVM，CRF 对数线性模型（log-linear model）对于有一系列输入数据$χ$ ，和一系列标签数据 $γ$，如何通过这些标注数据来求解 p(y|x)p(y|x) 呢？ 一般可以用对数线性模型来求解，也就是判别模型一般可以通过对数线性模型来建模，常见的最大熵模型、softmax、MEMM、CRF都使用了对数线性模型。 对数线性模型形如： $$p(y|x) = \\frac{p(x|y)p(y)}{\\sum_{y{}’ \\epsilon \\gamma} p(x|y{}’)p(y{}’)}$$ 其中有： 输入集合 $χ$ 标签集合 $γ$ $wϵRdwϵRd$ 是模型的参数 $x$, $yϵγ$ 特征函数 $f:χ×γ→Rd$ $f(x,y)$ 可认为对输入输出同时抽取特征 对数线性模型表示的意义为：在条件$x$以及参数$w$下发生$y$的概率，一般$w$是要学习的参数 分类问题和序列标注问题分类问题分类问题一般是对输入进行建模得到语义编码（如可以使用RNN，CNN进行编码）然后经过分类器将空间映射到分类空间上，最后可通过softmax将权重归一化得到映射到各个分类空间的概率，取概率最大的作为分类。 序列标注问题序列标注问题的重点在于学习序列位置之间的关系，然后解码出最大概率标签路径，比如有$K$个标签，当输入序列长度为$m$时，那么就有$K^m$条概率路径，序列标注问题是要从$K^m$条概率路径中寻找到概率最大的那条路径。NLP中常见的任务，如分词，词性标注，命名体识别都属于序列标注问题。 HMM之前已经说了HMM是生成模型，它引入了一阶马尔科夫假设：当前状态只与上一状态有关。结合这两点可以得到联合概率 $$p(x1…xm,s1…sn)=∏tp(st|st−1)p(xt|st)$$ $p(st|st−1)p(st|st−1)$称为状态间的转移概率，称为发射概率（由隐状态发射成显状态的概率） 由图可知HMM是一个有向图。 HMM的解码部分是求解：$argmaxs1…sm p(x1…xm,s1…sm)$, 通常采用Viterbi算法解码 HMM的特点HMM有两个独立性假设： 观测序列之间是独立的 当前状态仅依赖于先前的状态 MEMMMax Entropy Markov Model最大熵马尔可夫模型，在最大熵的基础上引入了一阶马尔科夫假设。 MEMM属于判别式模型，它要学习条件概率 $$p(s_{1}…s_{m}|x_{1}…x_{m}) = \\prod_{i=1}^{m}p(s_{i}|s_{1}…s_{i-1}, x_{1}…x_{m}) $$ 由于引入了一阶马尔科夫假设，故当前状态仅于前一状态有关 $$p(s_{1}…s_{m}|x_{1}…x_{m}) = \\prod_{i=1}^{m}p(s_{i}|s_{i-1}, x_{1}…x_{m})$$ 而最大熵模型是对数线性模型从而得到MEMM的模型表达式 $$p(s_{i}|s_{i-1}, x_{1}…x_{m}; w) = \\frac{exp(w \\cdot \\phi (x_{1}…x_{m}, i, s_{i-1}, s_{i}))}{ \\sum {s{}’ \\epsilon S} exp(w \\cdot \\phi (x{1}…x_{m}, i, s_{i-1}, s{}’) ) }$$ $\\phi (x_{1}…x_{m}, i, s_{i-1}, s{}’)$ 是一个特征函数，其中有： ii 代表当前被标记的位置 ss 先前的状态 s′s′ 当前的状态 故有 $$\\begin{align} p(s_{1}…s_{m}|x_{1}…x_{m}) &amp;= \\prod_{i=1}^{m} p(s_{i}|s_{i-1}, x_{1}…x_{m}) \\ &amp;= \\prod_{i=1}^{m}\\frac{exp(w\\cdot \\phi (x_{1}…x_{m}, i, s_{i-1}, s_{i}))}{\\sum {s{}’\\epsilon S}exp(w\\cdot \\phi (x{1}…x_{m}, i, s_{i-1}, s{}’))} \\ &amp;= \\frac{exp(\\sum {i} w\\cdot \\phi (x{1}…x_{m}, i, s_{i-1}, s_{i}))}{\\sum {s{}’\\epsilon S}exp(w\\cdot \\phi (x{1}…x_{m}, i, s_{i-1}, s{}’))} \\tag{1} \\end{align}$$ MEMM解码部分是计算 $arg \\underset{s_{1}…s_{m}}{max}\\ p( s_{1}…s_{m}|x_{1}…x_{m})$，为了好理解我做了一步步简化 $$\\begin{align} arg \\underset{s_{1}…s_{m}}{max}\\ p( s_{1}…s_{m}|x_{1}…x_{m}) &amp;= arg \\underset{s_{1}…s_{m}}{max}\\ \\prod_{i=1}^{m}p( s_{i}|s_{i-1} ,x_{1}…x_{m}) \\ &amp;= arg \\underset{s_{1}…s_{m}}{max}\\ p( s_{i}|s_{i-1}, x_{1}…x_{m} ) \\ &amp;= arg \\underset{s_{1}…s_{m}}{max}\\frac{exp(w\\cdot \\phi (x_{1}…x_{m}, i, s_{i-1}, s_{i}))}{\\sum {s{}’\\epsilon S}exp(w\\cdot \\phi (x{1}…x_{m}, i, s_{i-1}, s{}’))} \\ &amp;= arg \\underset{s_{1}…s_{m}}{max}\\ exp(w\\cdot \\phi (x_{1}…x_{m}, i, s_{i-1}, s_{i})) \\ &amp;= arg \\underset{s_{1}…s_{m}}{max}\\ w\\cdot \\phi (x_{1}…x_{m}, i, s_{i-1}, s_{i}) \\tag{2} \\end{align}$$ 也即是编码等价于求解公式(2)(2)，通常用viterbi算法求解，如果不采用viterbi求解它的算法复杂度是O(Km)O(Km) ，采用了viterbi算法可以降到O(mK2)O(mK2)，(KK 为隐状态数目, mm为观测序列长度) [ 如图，MEMM也是有向图模型。 MEMM的优点克服了HMM输出独立性问题，通过引入特征函数使得模型比HMM拥有更多信息，而且最大熵则从全局角度来建模，它“保留尽可能多的不确定性，在没有更多的信息时，不擅自做假设” MEMM的缺点标签偏置(labeling bias)问题，由于MEMM的当前状态只与当前观测以及上一状态有关，导致隐状态中有更少转移的状态拥有的转移概率普遍偏高（是不是一头雾水，再没有更好的解释之前只能继续一头雾水了）简单的说就是MEMM中概率最大路径更容易出现在转移少的状态中。 如何解决这个问题呢？引入全局化特征可以解决标签偏置问题，下文的CRF其实就在MEMM上加入全局化特征从而解决标签偏置问题 CRFConditional Random Forest条件随机场，这里主要讲解线性链（linear-chain）,这里不会牵扯太多概率图的东西（因为我也不会啊:&lt;）还是从对数线性模型出发。 首先要明确的一点是CRF也属于判别模型，所以和MEMM一样需要对 $$p(s1…sm|x1…xm)p(s1…sm|x1…xm)$$ 建模，CRF也和MEMM一样做了一阶马尔科夫假设，即当前状态只与上一状态有关，但是区别在于CRF的特征采用了全局特征，它把观测序列当做整体来看所以它的特征函数是全局的，它的特征函数为： $$φ(x1…xm,s1…sm)=∑j=1mϕ(x1…xm,j,sj−1,sj)φ(x1…xm,s1…sm)=∑j=1mϕ(x1…xm,j,sj−1,sj)$$ 其中 ϕϕ 和MEMM的特征函数是一致的，接下来的步骤和MEMM差不多了，只是特征函数变为了φφ，为了连续性在此再走一遍流程。 CRF的模型表达式为： $$p(si|si−1,x1…xm;w)=exp(w⋅φ(x1…xm,i,si−1,si))∑s′ϵSexp(w⋅φ(x1…xm,i,si−1,s′))p(si|si−1,x1…xm;w)=exp(w⋅φ(x1…xm,i,si−1,si))∑s′ϵSexp(w⋅φ(x1…xm,i,si−1,s′))$$ CRF的解码部分是计算 $argmaxs1…sm p(s1…sm|x1…xm)argmaxs1…sm p(s1…sm|x1…xm)$，一步步进行简化 $$argmaxs1…sm p(s1…sm|x1…xm)=argmaxs1…sm∏i=1mp(si|si−1,x1…xm)=argmaxs1…sm p(si|si−1,x1…xm)=argmaxs1…smexp(w⋅φ(x1…xm,i,si−1,si))∑s′ϵSexp(w⋅φ(x1…xm,i,si−1,s′))=argmaxs1…sm exp(w⋅φ(x1…xm,i,si−1,si))=argmaxs1…sm w⋅φ(x1…xm,i,si−1,si)=argmaxs1…sm ∑jmw⋅ϕ(x1…xm,i,si−1,si)(3)argmaxs1…sm p(s1…sm|x1…xm)=argmaxs1…sm∏i=1mp(si|si−1,x1…xm)=argmaxs1…sm p(si|si−1,x1…xm)=argmaxs1…smexp(w⋅φ(x1…xm,i,si−1,si))∑s′ϵSexp(w⋅φ(x1…xm,i,si−1,s′))=argmaxs1…sm exp(w⋅φ(x1…xm,i,si−1,si))=argmaxs1…sm w⋅φ(x1…xm,i,si−1,si)(3)=argmaxs1…sm ∑jmw⋅ϕ(x1…xm,i,si−1,si)$$ 对比一下(2)和(3)可知道CRF和MEMM的区别。和MEMM一样，一般采用viterbi算法来进行解码。 由图可知线性链CRF是无向图模型。 CRF的优点克服了HMM的输出独立性假设问题以及MEMM的标注偏置问题。 后记HMM、MEMM属于有向图模型，贝叶斯网络一般属于有向图。而CRF属于马尔科夫网络属于无向图。所以它们本身属于统计概率图（PGM）的一部分，要想真正弄懂之间的原理和区别还需要系统的学习PGM。 另，由Refrence中可知本文大量引用了Michael Collins教授的tutorial,MC的tutorial通俗易懂，但是有些地方做了简化，如果不详细说明有时也会一头雾水，所以本文主要做了些翻译和修补工作。 Refrence[1] http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf [2] http://www.cs.columbia.edu/~mcollins/loglinear.pdf [3] http://www.cs.columbia.edu/~mcollins/crf.pdf [4] https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;context=cis_papers","link":"","categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://Yansz.github.io/tags/NLP/"}]},{"title":"Word2vec理解","date":"2019-01-14T22:50:10.000Z","path":"2019/01/15/Word2vec理解/","text":"Word2Vec 原理讲解1.介绍 2013年，Google开源了一款用于词向量计算的工具—— word2vec，引起了工业界和学术界的关注。 提到word2vec 算法或模型，其实指的是其背后用于计算 word vector 的 CBoW 模型和 Skip-gram 模型 很多人以为 word2vec 指的是一个算法或模型，实际上并不正确。 因此通过 Word2Vec 技术 输出的词向量可以被用来做很多NLP相关的工作，比如聚类、找同义词、词性分析等等. 适用场景 cbow适用于小规模，或者主题比较散的语料，这是因为他的向量产生只跟临近的字有关系，更远的语料并没有被采用。 而相反的skip-gram可以处理基于相同语义，义群的一大批语料。 1.1.Statistical Language Model在深入word2vec算法的细节之前，首先回顾一下自然语言处理中的一个基本问题：如何计算一段文本序列在某种语言下出现的概率？之所为称其为一个基本问题，是因为它在很多NLP任务中都扮演着重要的角色。例如，在机器翻译的问题中，如果我们知道了目标语言中每句话的概率，就可以从候选集合中挑选出最合理的句子做为翻译结果返回。 统计语言模型给出了这一类问题的一个基本解决框架。对于一段文本序列$S=w_1, w_2, … , w_T$，它的概率可以表示为： $$P(S)=P(w_1, w_2, …, w_T)=\\prod_{t=1}^Tp(w_t|w_1, w_2, …, w_{t-1})$$ 即将序列的联合概率转化为一系列条件概率的乘积。问题变成了如何去预测这些给定previous words下的条件概率。 由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么卵用。我们更多的是采用其简化版本——Ngram模型： $$p(w_t|w_1,w_2,…,w_{t-1})$$ 常见的如bigram模型（$n=2$）和trigram模型（$n=3$）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑$n&gt;3$的模型。 我们可以用最大似然法去求解Ngram模型的参数，这等价于去统计每个Ngram的条件词频。 为了避免统计中出现的零概率问题（一段从未在训练集中出现过的Ngram片段会使得整个序列的概率为00），人们基于原始的Ngram模型进一步发展出了back-off trigram模型（用低阶的bigram和unigram代替零概率的trigram）和interpolated trigram模型（将条件概率表示为unigram、bigram、trigram三者的线性函数）。 1.2. Distributed Representation不过，Ngram模型仍有其局限性。首先，由于参数空间的爆炸式增长，它无法处理更长程的context（N&gt;3）。其次，它没有考虑词与词之间内在的联系性。例如，考虑”the cat is walking in the bedroom”这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率。然而， Ngram模型做不到。 这是因为，Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量（除了一个词典索引的下标对应的方向上是11，其余方向上都是00）。例如，对于一个大小为55的词典：{“I”, “love”, “nature”, “luaguage”, “processing”}，“nature”对应的one-hot向量为：[0,0,1,0,0]。显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（the curse of dimensionality） 于是，人们就自然而然地想到，能否用一个连续的稠密向量去刻画一个word的特征呢？这样，我们不仅可以直接刻画词与词之间的相似度，还可以建立一个从向量到概率的平滑函数模型，使得相似的词向量可以映射到相近的概率空间上。这个稠密连续向量也被称为word的distributed representation。 事实上，这个概念在信息检索（Information Retrieval）领域早就已经被广泛地使用了。只不过，在IR领域里，这个概念被称为向量空间模型（Vector Space Model，以下简称VSM）。 VSM是基于一种Statistical Semantics Hypothesis：语言的统计特征隐藏着语义的信息（Statistical pattern of human word usage can be used to figure out what people mean）。例如，两篇具有相似词分布的文档可以被认为是有着相近的主题。这个Hypothesis有很多衍生版本。其中，比较广为人知的两个版本是Bag of Words Hypothesis和Distributional Hypothesis。前者是说，一篇文档的词频（而不是词序）代表了文档的主题；后者是说，上下文环境相似的两个词有着相近的语义。后面我们会看到，word2vec算法也是基于Distributional的假设。 那么，VSM是如何将稀疏离散的one-hot词向量映射为稠密连续的distributional representation的呢？ 简单来说，基于Bag of Words Hypothesis，我们可以构造一个term-document矩阵A：矩阵的行$A_i,:$对应着词典里的一个word；矩阵的列$A:,j$对应着训练语料里的一篇文档；矩阵里的元素$A_ij$代表着word $w_i$在文档$D_j$中出现的次数（或频率）。那么，我们就可以提取行向量做为word的语义向量（不过，在实际应用中，我们更多的是用列向量做为文档的主题向量）。 类似地，我们可以基于Distributional Hypothesis构造一个word-context的矩阵。此时，矩阵的列变成了context里的word，矩阵的元素也变成了一个context窗口里word的共现次数。 注意，这两类矩阵的行向量所计算的相似度有着细微的差异：term-document矩阵会给经常出现在同一篇document里的两个word赋予更高的相似度；而word-context矩阵会给那些有着相同context的两个word赋予更高的相似度。后者相对于前者是一种更高阶的相似度，因此在传统的信息检索领域中得到了更加广泛的应用。 不过，这种co-occurrence矩阵仍然存在着数据稀疏性和维度灾难的问题。为此，人们提出了一系列对矩阵进行降维的方法（如LSI／LSA等）。这些方法大都是基于SVD的思想，将原始的稀疏矩阵分解为两个低秩矩阵乘积的形式。 1.3. Neural Network Language Model接下来，让我们回到对统计语言模型的讨论。鉴于Ngram等模型的不足，2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model。在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM），并首次提出了word embedding的概念（虽然没有叫这个名字），从而奠定了包括word2vec在内后续研究word representation learning的基础。 NNLM模型的基本思想可以概括如下： 假定词表中的每一个word都对应着一个连续的特征向量； 假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率； 同时学习词向量的权重和概率模型里的参数。 值得注意的一点是，这里的词向量也是要学习的参数。 在03年的论文里，Bengio等人采用了一个简单的前向反馈神经网络$f(w_{t-n+1},…,w_{t})$ 来拟合一个词序列的条件概率$p(w_t|w_1,w_2,…,w_{t-1})$。整个模型的网络结构见下图： 我们可以将整个模型拆分成两部分加以理解： 首先是一个线性的embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是embedding向量的维度（一个先验参数）。C矩阵里存储了要学习的word vector。 其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。通过将embedding层输出的N−1个词向量映射为一个长度为V 的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估： $$p(w_i|w_1,w_2,…,w_{t-1}) \\approx f(w_i, w_{t-1}, …, w_{t-n+1}) = g(w_i, C(w_{t-n+1}), …, C(w_{t-1}))$$ 我们可以通过最小化一个cross-entropy的正则化损失函数来调整模型的参数$\\theta$： $$L(\\theta)=\\frac{1}{T}\\sum_t{\\log{f(w_t, w_{t-1}, …, w_{t-n+1})}}+R(\\theta)$$ 其中，模型的参数$\\theta$包括了embedding层矩阵C的元素，和前向反馈神经网络模型g里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）。计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。 然而，抛却复杂的参数空间，我们不禁要问，为什么这样一个简单的模型会取得巨大的成功呢？ 仔细观察这个模型就会发现，它其实在同时解决两个问题：一个是统计语言模型里关注的条件概率$p(w_t|context)$的计算；一个是向量空间模型里关注的词向量的表达。而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。另一方面，以条件概率$p(w_t|context)$为学习目标去更新词向量的权重，具有更强的导向性，同时也与VSM里的Distributional Hypothesis不谋而合。 2. NNLM存在的几个问题第一个问题是，同Ngram模型一样，NNLM模型只能处理定长的序列。在03年的论文里，Bengio等人将模型能够一次处理的序列长度N提高到了5，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。 因此，Mikolov等人在2010年提出了一种RNNLM模型，用递归神经网络代替原始模型里的前向反馈神经网络，并将embedding层与RNN里的隐藏层合并，从而解决了变长序列的问题。 另一个问题就比较严重了。NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个impossible mission。 这时候，还是那个Mikolov站了出来。他注意到，原始的NNLM模型的训练其实可以拆分成两个步骤： 用一个简单模型训练出连续的词向量； 基于词向量的表达，训练一个连续的Ngram神经网络模型。而NNLM模型的计算瓶颈主要是在第二步。 2.1. CBoW 模型（Continuous Bag-of-Words Model）如果我们只是想得到word的连续特征向量，是不是可以对第二步里的神经网络模型进行简化呢？ Mikolov是这么想的，也是这么做的。他在2013年一口气推出了两篇paper，并开源了一款计算词向量的工具——至此，word2vec横空出世，主角闪亮登场。 下面简单剖析下word2vec算法的原理。有了前文的基础，理解word2vec算法就变得很简单了。 首先，我们对原始的NNLM模型做如下改造： 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的embedding layer与输出层的softmax layer连接； 忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer； 将future words纳入上下文环境 CBOW模型的输入是某个词A周围的n个单词的词向量之和，输出是词A本身的词向量. 从数学上看，CBoW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。 CBoW模型依然是从context对target word的预测中学习到词向量的表达。反过来，我们能否从target word对context的预测中学习到word vector呢？答案显然是可以的： 2.2. Skip-gram 模型 这个模型被称为Skip-gram模型（名称源于该模型在训练时会对上下文环境里的word进行采样）。 如果将Skip-gram模型的前向计算过程写成数学形式，我们得到： $$p(w_o|w_i)=\\frac{e^{U_o \\cdot V_i}}{\\sum_j{e^{U_j \\cdot V_i}}}$$ 其中，$V_i$是embedding层矩阵里的列向量，也被称为$w_i$的input vector。$U_j$是softmax层矩阵里的行向量，也被称为$w_j$的output vector。 因此，Skip-gram模型的本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化。我们要学习的模型参数正是这两类词向量。 然而，直接对词典里的VV个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。 skip-gram与CBOW相比，只有细微的不同。skip-gram的输入是当前词的词向量，而输出是周围词的词向量。 Skip-gram 模型: 能够根据词本身来预测周围有哪些词. 接下来介绍这两种优化算法。 2.2.1. Hierarchical Softmax层次Softmax的方法最早由Bengio在05年引入到语言模型中。它的基本思想是将复杂的归一化概率分解为一系列条件概率乘积的形式： $$p(v|context)=\\prod_{i=1}^m{p(b_i(v)|b_1(v), …, b_{i-1}(v), context)}$$ 其中，每一层条件概率对应一个二分类问题，可以通过一个简单的逻辑回归函数去拟合。这样，我们将对个词的概率归一化问题，转化成了对个词的概率拟合问题。 我们可以通过构造一颗分类二叉树来直观地理解这个过程。首先，我们将原始字典$D$划分为两个子集$D_1$、$D_2$，并假设在给定context下，target word属于子集$D_1$的概率$p(w_t \\in D_1|context)$服从logistical function的形式： $$p(w_t \\in D_1|context)=\\frac{1}{1+e^{-U_{D_{root}} \\cdot V_{w_t}}}$$ 其中，$U_{D_{root}}$和$V_{w_t}$都是模型的参数。 接下来，我们可以对子集$D_1$和$D_2$进一步划分。重复这一过程，直到集合里只剩下一个word。这样，我们就将原始大小为$V$的字典$D$转换成了一颗深度为$\\log V$的二叉树。树的叶子节点与原始字典里的word一一对应；非叶节点则对应着某一类word的集合。显然，从根节点出发到任意一个叶子节点都只有一条唯一路径——这条路径也编码了这个叶子节点所属的类别。 同时，从根节点出发到叶子节点也是一个随机游走的过程。因此，我们可以基于这颗二叉树对叶子节点出现的似然概率进行计算。例如，对于训练样本里的一个target word $w_t$，假设其对应的二叉树编码为{1,0,1,…,1}，则我们构造的似然函数为： $$p(w_t|context)=p(D_1=1|context)p(D_2=0|D_1=1)\\dots p(w_t|D_k=1)$$ 乘积中的每一项都是一个逻辑回归的函数。 我们可以通过最大化这个似然函数来求解二叉树上的参数——非叶节点上的向量，用来计算游走到某一个子节点的概率。 层次Softmax是一个很巧妙的模型。它通过构造一颗二叉树，将目标概率的计算复杂度从最初的$V$降低到了$\\log V$的量级。不过付出的代价是人为增强了词与词之间的耦合性。例如，一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响。因此，构造一颗有意义的二叉树就显得十分重要。实践证明，在实际的应用中，基于Huffman编码的二叉树可以满足大部分应用场景的需求。 2.2.2. Negative Sampling负采样的思想最初来源于一种叫做Noise-Contrastive Estimation的算法，原本是为了解决那些无法归一化的概率模型的参数预估问题。与改造模型输出概率的层次Softmax算法不同，NCE算法改造的是模型的似然函数。 以Skip-gram模型为例，其原始的似然函数对应着一个Multinomial的分布。在用最大似然法求解这个似然函数时，我们得到一个cross-entropy的损失函数： $$J(\\theta)=-\\frac{1}{T}\\sum_{t=1}^T{\\sum_{-c \\leq j \\leq c, j \\neq 0}{\\log p(w_{t+j}|w_t)}}$$式中的$p(w_{t+j}|w_t)$是一个在整个字典上归一化了的概率。 而在NCE算法中，我们构造了这样一个问题：对于一组训练样本&lt;context, word&gt;，我们想知道，target word的出现，是来自于context的驱动，还是一个事先假定的背景噪声的驱动？显然，我们可以用一个逻辑回归的函数来回答这个问题： $$p(D=1|w, context)=\\frac{p(w|context)}{p(w|context)+kp_n(w)}=\\sigma (\\log p(w|context) - \\log kp_n(w))$$ 这个式子给出了一个target word $w$来自于context驱动的概率。其中，$k$是一个先验参数，表明噪声的采样频率。$p(w|context)$是一个非归一化的概率分布，这里采用softmax归一化函数中的分子部分。$p_n(w)$则是背景噪声的词分布。通常采用word的unigram分布。 通过对噪声分布的kk采样，我们得到一个新的数据集：&lt;context, word, label&gt;。其中，label标记了数据的来源（真实数据分布还是背景噪声分布？）。在这个新的数据集上，我们就可以用最大化上式中逻辑回归的似然函数来求解模型的参数。 而Mikolov在2013年的论文里提出的负采样算法， 是NCE的一个简化版本。在这个算法里，Mikolov抛弃了NCE似然函数中对噪声分布的依赖，直接用原始softmax函数里的分子定义了逻辑回归的函数，进一步简化了计算： $$p(D=1|w_o, w_i)=\\sigma (U_o \\cdot V_i)$$此时，模型相应的目标函数变为： $$J(\\theta) = \\log \\sigma(U_o \\cdot V_i) + \\sum_{j=1}^k{E_{w_j \\sim p_n(w)}[\\log \\sigma(- U_j \\cdot V_i)]}$$ 除了这里介绍的层次Softmax和负采样的优化算法，Mikolov在13年的论文里还介绍了另一个trick：下采样（subsampling）。其基本思想是在训练时依概率随机丢弃掉那些高频的词： $$p_{discard}(w) = 1 - \\sqrt{\\frac{t}{f(w)}}$$其中，$t$是一个先验参数，一般取为$10^{-5}$。是在语料中出现的频率。 实验证明，这种下采样技术可以显著提高低频词的词向量的准确度。 3. 模型的应用word2vec模型可以被应用于机器翻译和推荐系统领域。 3.1. Machine Translation与后来提出的在sentence level上进行机器翻译的RNN模型不同，word2vec模型主要是用于词粒度上的机器翻译。 具体来说，我们首先从大量的单语种语料中学习到每种语言的word2vec表达，再借助一个小的双语语料库学习到两种语言word2vec表达的线性映射关系$W$。构造的损失函数为： $$J(W)=\\sum_{i=1}^n{||Wx_i - z_i||^2}$$ 在翻译的过程中，我们首先将源语言的word2vec向量通过矩阵$W$映射到目标语言的向量空间上；再在目标语言的向量空间中找出与投影向量距离最近的word做为翻译的结果返回。 其原理是，不同语言学习到的word2vec向量空间在几何上具有一定的同构性。映射矩阵$W$本质上是一种空间对齐的线性变换。 3.2. Item2Vec本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。 比如，推荐系统领域的协同过滤算法。 协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。 参考资料： [1]: Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013, January 17). Efficient Estimation of Word Representations in Vector Space. arXiv.org. [2]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013, October 17). Distributed Representations of Words and Phrases and their Compositionality. arXiv.org. [3]: Bengio, Y., Ducharme, R., Vincent, P., &amp; Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155. [4]: Turney, P. D., &amp; Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1). [5]. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537.","link":"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://Yansz.github.io/categories/NLP/"}],"tags":[{"name":"Vec","slug":"Vec","permalink":"https://Yansz.github.io/tags/Vec/"}]},{"title":"NER中的两种DL模型","date":"2019-01-10T21:00:00.000Z","path":"2019/01/11/NER中的两种DL模型/","text":"命名实体识别（Named Entity Recognition，NER）是NLP中一项非常基础的任务。NER是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。 本篇笔记主要记录目前NER最常用的两种深度学习模型，LSTM+CRF和Dilated-CNN。 1 LSTM+CRF 在NLP领域，有那么一段时间，LSTM是“最红”的特征抽取器，NER中也使用LSTM来进行特征抽取。 上述架构图中采用的双向 LSTM 架构和之前的 LSTM 架构略有不同，具体公式如下： $${\\boldsymbol{i}{t}=\\sigma\\left(\\boldsymbol{W}{x i} \\boldsymbol{x}{t}+\\boldsymbol{W}{h i} \\boldsymbol{h}{t-1}+\\boldsymbol{W}{c i} \\boldsymbol{c}{t-1}+\\boldsymbol{b}{i}\\right)} \\{\\boldsymbol{c}{t}=\\left(1-\\boldsymbol{i}{t}\\right) \\odot \\boldsymbol{c}{t-1}+\\boldsymbol{i}{t} \\odot \\tanh \\left(\\boldsymbol{W}{x c} \\boldsymbol{x}{t}+\\boldsymbol{W}{h c} \\boldsymbol{h}{t-1}+\\boldsymbol{b}{c}\\right)} \\{\\boldsymbol{o}{t}=\\sigma\\left(\\boldsymbol{W}{x o} \\boldsymbol{x}{t}+\\boldsymbol{W}{h o} \\boldsymbol{h}{t-1}+\\boldsymbol{W}{c o} \\boldsymbol{c}{t}+\\boldsymbol{b}{o}\\right)} \\{\\boldsymbol{h}{t}=\\boldsymbol{o}{t} \\odot \\tanh \\left(\\boldsymbol{c}{t}\\right)}\\end{array}$$ 对比前述 LSTM 表示方式可见，这里采用了 ($1− i_t$) 替换遗忘门，同时输入门依赖 LSTM记忆单元历史信息 $c_{t−1}$，输出门依赖于更新后的当前 LSTM 记忆单元信息 $c_t$。给定上下文下，每个词的特征表示由双向 LSTM 各自输出的 $h_t$ 拼接而成$\\boldsymbol{h}{t}=\\left[\\overrightarrow{\\boldsymbol{h}}{t} ; \\overleftarrow{\\boldsymbol{h}_{t}}\\right]$ 。给定每个词当前上下文下的向量表示后，即可以用于进行命名实体识别。 假设命名实体中有两类实体：人名 (PER)，地名 (LOC)，分别采用 B 和 I 两类标签表示词在命名实体中处于开始和中间位置，用 O 表示其他非命名实体词，则命名实体识别问题转化为序列标注问题，该问题可以采用 CRF 模型进行解决。给定序列 $$\\boldsymbol{H}=\\left(\\boldsymbol{h}{1}, \\boldsymbol{h}{2}, \\ldots, \\boldsymbol{h}_{n}\\right)$$ 期望生成上述序列对应的标注序列： $$y=\\left(y_{1}, y_{2}, \\ldots, y_{n}\\right)$$ 采用 CRF 表示方式，序列对应的矩阵$H$和标注对应的向量 $y$联合权重表示形式如下：$$s(\\boldsymbol{H}, \\boldsymbol{y})=\\sum_{i=0}^{n} A_{y_{i}, y_{i+1}}+\\sum_{i=1}^{n} P_{i, y_{i}}$$分析一下这个模型，看数据的流转和各层的作用。 1.embedding layer 将中文转化为字向量，获得输入embedding 2.将embedding输入到BiLSTM层，进行特征提取（编码），得到序列的特征表征，logits。 3.logits需要解码，得到标注序列。将其输入到解码CRF层，获得每个字的序列。 该模型的重点有两个： 1. 引入双向LSTM层作为特征提取工具，LSTM拥有较强的长序列特征提取能力，是个不错的选择。双向LSTM，在提取某个时刻特征时，能够利用该时刻之后的序列的信息，无疑能够提高模型的特征提取能力。 2. 引入CRF作为解码工具。中文输入经过双向LSTM层的编码之后，需要能够利用编码到的丰富的信息，将其转化成NER标注序列。通过观察序列，预测隐藏状态序列，CRF无疑是首选。 2 ID-CNN+CRF 对于膨胀卷积先做一个简单的介绍。 Dilated/Atrous Convolution（中文叫做空洞卷积或者膨胀卷积)）或者是Convolution with holes。从字面上就很好理解，就是在标准的卷积里注入空洞，以此来增加感受野。 如上图所示，相比原来的正常卷积，膨胀卷积多了一个超参数，称之为膨胀率（dilation rate），指的是kernel的间隔数量(例如，正常的卷积是膨胀率是1)。 “膨胀”的好处是，不做池化，不损失信息的情况下，增大了感受野，让每个卷积输出都包含较大范围的信息。 《Fast and Accurate Entity Recognition with Iterated Dilated Convolutions》一文中提出在NER任务中，引入膨胀卷积，一方面可以引入CNN并行计算的优势，提高训练和预测时的速度；另一方面，可以减轻CNN在长序列输入上特征提取能力弱的劣势。 具体使用时，dilated width会随着层数的增加而指数增加。这样随着层数的增加，参数数量是线性增加的，而感受野却是指数增加的，这样就可以很快覆盖到全部的输入数据。 LSTM+CRF和ID-CNN+CRF两种模型其实都是同一个架构：深度学习特征提取+CRF解码。现在绝大部分的NER任务都是采用这样的一套框架。","link":"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://Yansz.github.io/categories/NLP/"}],"tags":[{"name":"NER","slug":"NER","permalink":"https://Yansz.github.io/tags/NER/"}]},{"title":"词嵌入方法","date":"2019-01-09T21:00:00.000Z","path":"2019/01/10/词嵌入/","text":"引言对于自然语言处理任务，我们不能直接使用纯文本字符串作为输出，而是需要将文本字符转化为连续值的向量。 词嵌入的主要思想是将文本转换为较低维度空间的矢量表示。对于这种转化后的向量有两个重要的要求： 较低维度空间：尽量降低编码词向量产生的稀疏性。 保证上下文相似性：为了使语言模型更好的预测文本的含义。 总的来说，词嵌入就是一种从文本语料库构建低维向量表示的方法，可以保留单词的上下文相似性。 介绍Word2Vec现在我们知道它是什么，接下来讨论词嵌入如何工作。如果您曾遇到降维，那么您通常会使用无监督学习算法来实现降维。这是最著名的算法之一用于生成单词嵌入的方法：word2vec。 实际上，有两种实现word2vec的方式，即CBOW（连续词袋）和Skip-gram。 CBOW：以上下文单词作为输入，预测目标单词（中心词） Skip-gram: 以目标单词作为输入，预测单词周围的上下文。 输入的单词使用one-hot编码。这将进入具有线性单元的隐藏层，然后进入softmax层进行预测。这里的想法是训练隐藏层权重矩阵，以找到我们单词的有效表示形式。该权重矩阵通常称为嵌入矩阵，可以作为查询表进行查询。 嵌入矩阵的大小为单词数乘以隐藏层中神经元的数量（嵌入大小）。嵌入的大小（即隐藏层神经元的个数，因此表示单词之间相似度的特征的数量）往往比词汇表中唯一单词的总数小得多。因此，如果您有10,000个单词和300个隐藏层神经元，则矩阵的大小将为10,000×300（因为输入编码为独热编码向量）。一旦计算出字向量，就可以对结果矩阵的相应行进行快速O（1）查找。最终每个单词都会产生一个与之关联的一个向量。 注意：所使用的嵌入大小设计是一个折衷：更多功能意味着额外的计算复杂性，因此运行时间更长，但是还允许更多的精细表示和可能更好的模型。 词嵌入的一个有趣特征是，由于它们是单词之间上下文相似性的数字表示，因此可以进行算术操作。经典示例是从“男人”中减去“国王”的“概念”，并添加“女人”的概念。答案将取决于您的训练方式，但您可能会看到其中一项最佳成绩是“女王”一词。这篇帖子解释了原因。 补充另一个值得了解的单词嵌入算法是GloVe，它通过累加共现次数来工作略有不同（请参阅GloVe与word2vec有何不同？） 那么和深度学习有什么关系呢？ 请记住，Word2vec只是两层浅层神经网络，因此它本身并不是深度学习的示例。但是，诸如Word2vec和GloVe之类的技术可以将原始文本转换成深层网络可以理解的数字形式，例如，使用带有词嵌入的递归神经网络。","link":"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://Yansz.github.io/categories/NLP/"}],"tags":[]},{"title":"NER的概念","date":"2019-01-09T21:00:00.000Z","path":"2019/01/10/NER的概念/","text":"命名实体识别（Named Entity Recognition，NER）是NLP一项基础任务。是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。NER是信息提取、问答系统、句法分析、机器翻译等众多NLP任务的重要基础工具。命名实体识别的准确度，决定了下游任务的效果。 1 命名实体识别的概念要解释NER，首先要理解什么算是实体，实体就是具有特定意义的实例。 在学术上NER所涉及的命名实体一般包括3大类（实体类，时间类，数字类）和7小类（人名、地名、组织机构名、时间、日期、货币、百分比）。 实际应用中，NER模型通常只要识别出人名、地名、组织机构名、日期时间即可，一些系统还会给出专有名词结果（比如缩写、会议名、产品名等）。货币、百分比等数字类实体可通过正则搞定。另外，在一些应用场景下会给出特定领域内的实体，如书名、歌曲名、期刊名等。 所谓实体识别，就是一个从语料中提取出想要获取到的实体类型的过程。 2.研究现状：命名实体识别只是在有限的文本类型（主要是新闻语料中）和实体类别（主要是人名、地名、组织机构名）中取得了不错的效果；与其他信息检索领域相比，实体命名评测预料较小，容易产生过拟合；命名实体识别更侧重高召回率，但在信息检索领域，高准确率更重要；通用的识别多种类型的命名实体的系统性能很差。 3.命名实体识别的数据标注方式NER是一种序列标注问题，因此他们的数据标注方式也遵照序列标注问题的方式，主要是BIO和BIOES两种。这里直接介绍BIOES，明白了BIOES，BIO也就掌握了。先列出来BIOES分别代表的含义： B，即Begin，表示开始 I，即Intermediate，表示中间 E，即End，表示结尾 S，即Single，表示单个字符 O，即Other，表示其他，用于标记无关字符例：使用BIOES对“我在人民广场吃着炸鸡”这句话，进行标注，每个字对应的结果就是：[B-PER，E-PER，O, B-LOC，I-LOC，I-LOC，E-LOC，O，O，O，O] 4.命名实体识别的方法介绍1）HMM，MEMM和CRF等机器学习算法HMM和CRF很适合用来做序列标注问题，早期很多的效果较好的成果，都是出自这两个模型。两种模型在序列标注问题中应用：【NLP】用于语音识别、分词的隐马尔科夫模型HMM【NLP】用于序列标注问题的条件随机场（Conditional Random Field, CRF） 2）LSTM+CRF 目前做NER比较主流的方法就是采用LSTM作为特征抽取器，再接一个CRF层来作为输出层，如下图所示： 3）CNN+CRF CNN虽然在长序列的特征提取上有弱势，但是CNN模型可有并行能力，有运算速度快的优势。膨胀卷积的引入，使得CNN在NER任务中，能够兼顾运算速度和长序列的特征提取。 4）BERT+（LSTM）+CRF BERT中蕴含了大量的通用知识，利用预训练好的BERT模型，再用少量的标注数据进行FINETUNE是一种快速的获得效果不错的NER的方法。","link":"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://Yansz.github.io/categories/NLP/"}],"tags":[{"name":"NER","slug":"NER","permalink":"https://Yansz.github.io/tags/NER/"}]}]